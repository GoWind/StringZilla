{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exploring Rabin-Karp-style Min-Hash Fingerprinting\n",
                "\n",
                "This document showcases the differences between different numeric types that one can use to implement a Rabin-Karp-style min-hash fingerprinting algorithm.\n",
                "It answers several important questions:\n",
                "\n",
                "- How to use floating-point numbers for a traditionally integer-based task - \"hashing\"?\n",
                "- How to properly compose many such hash functions to maximize the quality of fingerprints?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Rabin-Karp Rolling Hashing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Generator\n",
                "\n",
                "\n",
                "def rabin_karp_ints(\n",
                "    s: str,\n",
                "    window_width: int,\n",
                "    multiplier: int,\n",
                "    modulo: int,\n",
                "    alphabet_size: int = 256,\n",
                ") -> Generator[int, None, None]:\n",
                "    \"\"\"Return the rolling polynomial hashes of every length-`window_width` substring of `s`\"\"\"\n",
                "    \n",
                "    assert window_width > 0, \"Window width must be positive\"\n",
                "    assert multiplier > 0, \"Multiplier must be positive\"\n",
                "    assert modulo > 0, \"Modulo must be positive\"\n",
                "    assert multiplier < modulo, \"Multiplier must be less than modulo\"\n",
                "\n",
                "    if len(s) < window_width:\n",
                "        return\n",
                "\n",
                "    current_hash: int = 0\n",
                "    for char in s[:window_width]:\n",
                "        new_term = ord(char) + 1\n",
                "        assert new_term <= alphabet_size, \"Pass correct `alphabet_size`\"\n",
                "        current_hash = (current_hash * multiplier + new_term) % modulo\n",
                "    yield current_hash\n",
                "\n",
                "    discarding_multiplier: int = pow(multiplier, window_width - 1, modulo)\n",
                "    total_hashes = len(s) - window_width + 1\n",
                "    for i in range(1, total_hashes):  # First hash is already yielded\n",
                "        old_term = ord(s[i - 1]) + 1\n",
                "        new_term = ord(s[i + window_width - 1]) + 1\n",
                "        \n",
                "        # Remove leftmost char and add the new rightmost one.\n",
                "        # All operations must be modulo `modulo`, but assuming the infinite precision of integers,\n",
                "        # we don't care in this draft.\n",
                "        current_hash = (current_hash - old_term * discarding_multiplier) % modulo\n",
                "        current_hash = (current_hash * multiplier + new_term) % modulo\n",
                "        yield current_hash\n",
                "\n",
                "\n",
                "# Quick sanity-check\n",
                "assert list(rabin_karp_ints(\"abcd\", 3, 31, 1_000_000_007)) == [\n",
                "    next(rabin_karp_ints(\"abc\", 3, 31, 1_000_000_007)),\n",
                "    next(rabin_karp_ints(\"bcd\", 3, 31, 1_000_000_007)),\n",
                "]\n",
                "assert list(rabin_karp_ints(\"abcdefdhijklmnopqr\", 17, 31, 65521)) == [\n",
                "    next(rabin_karp_ints(\"abcdefdhijklmnopq\", 17, 31, 65521)),\n",
                "    next(rabin_karp_ints(\"bcdefdhijklmnopqr\", 17, 31, 65521)),\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Rabin-Karp Rolling Hashing via Floats\n",
                "\n",
                "The Python's `int` type is unbounded, so it can be used to implement the Rabin-Karp rolling hash algorithm without worrying about overflow.\n",
                "It is, however, insanely expensive to use, and doesn't allow us to explore optimization opportunities.\n",
                "The `float`, on the other hand, is just a double-precision IEEE 754 floating-point number, which can exactly represent 52-bit integers!\n",
                "Thus, we can convert our arithmetic to use `float`s, if we guarantee, that no intermediate result will exceed that limit."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Generator\n",
                "\n",
                "LARGEST_INTEGRAL_FLOAT: float = 4503599627370495.0\n",
                "\n",
                "\n",
                "def rabin_karp_floats(\n",
                "    s: str,\n",
                "    window_width: int,\n",
                "    multiplier: int,\n",
                "    modulo: int,\n",
                "    alphabet_size: int = 256,\n",
                ") -> Generator[int, None, None]:\n",
                "    \"\"\"Return the rolling polynomial hashes of every length-`window_width` substring of `s`\"\"\"\n",
                "\n",
                "    assert window_width > 0, \"Window width must be positive\"\n",
                "    assert multiplier > 0, \"Multiplier must be positive\"\n",
                "    assert modulo > 0, \"Modulo must be positive\"\n",
                "    assert multiplier < modulo, \"Multiplier must be less than modulo\"\n",
                "\n",
                "    if len(s) < window_width:\n",
                "        return\n",
                "\n",
                "    multiplier = float(multiplier)\n",
                "    modulo = float(modulo)\n",
                "    assert (\n",
                "        modulo < LARGEST_INTEGRAL_FLOAT\n",
                "    ), \"Modulo can't exceed the largest integral float value\"\n",
                "\n",
                "    # Ensure, we won't overflow the floating-point representation\n",
                "    largest_post_modulo = modulo - 1\n",
                "    max_possible_term = alphabet_size\n",
                "    assert (\n",
                "        largest_post_modulo * multiplier + max_possible_term <= LARGEST_INTEGRAL_FLOAT\n",
                "    ), \"Will overflow\"\n",
                "\n",
                "    # All of the operations will happen with a modulo:\n",
                "    def mul_mod(a: float, b: float) -> float:\n",
                "        return (a * b) % modulo\n",
                "\n",
                "    def add_mod(a: float, b: float) -> float:\n",
                "        return (a + b) % modulo\n",
                "\n",
                "    def sub_mod(a: float, b: float) -> float:\n",
                "        return (a - b) % modulo\n",
                "\n",
                "    # Precompute the discarding multiplier\n",
                "    discarding_multiplier: float = 1.0\n",
                "    for _ in range(window_width - 1):\n",
                "        discarding_multiplier = mul_mod(discarding_multiplier, multiplier)\n",
                "\n",
                "    # Handle the first window - without dropping any characters\n",
                "    current_hash: float = 0.0\n",
                "    for char in s[:window_width]:\n",
                "        new_term = float(ord(char) + 1)\n",
                "        assert new_term <= alphabet_size, \"Pass correct `alphabet_size`\"\n",
                "        current_hash = add_mod(mul_mod(current_hash, multiplier), new_term)\n",
                "    yield int(current_hash)\n",
                "\n",
                "    # Roll through the rest of the string\n",
                "    total_hashes = len(s) - window_width + 1\n",
                "    for i in range(1, total_hashes):  # First hash is already yielded\n",
                "        old_term = float(ord(s[i - 1]) + 1)\n",
                "        new_term = float(ord(s[i + window_width - 1]) + 1)\n",
                "\n",
                "        # Remove leftmost char and add the new rightmost one.\n",
                "        current_hash = sub_mod(current_hash, mul_mod(old_term, discarding_multiplier))\n",
                "        current_hash = add_mod(mul_mod(current_hash, multiplier), new_term)\n",
                "        yield int(current_hash)\n",
                "\n",
                "\n",
                "# Quick sanity-check\n",
                "assert list(rabin_karp_floats(\"abcd\", 3, 31, 1_000_000_007)) == [\n",
                "    next(rabin_karp_floats(\"abc\", 3, 31, 1_000_000_007)),\n",
                "    next(rabin_karp_floats(\"bcd\", 3, 31, 1_000_000_007)),\n",
                "]\n",
                "assert list(rabin_karp_floats(\"abcdefdhijklmnopqr\", 17, 31, 65521)) == [\n",
                "    next(rabin_karp_floats(\"abcdefdhijklmnopq\", 17, 31, 65521)),\n",
                "    next(rabin_karp_floats(\"bcdefdhijklmnopqr\", 17, 31, 65521)),\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's load some data and ensure that the outputs are identical between the `int` and `float` implementations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "dataset_directory = Path(\"..\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "textual_dataset_path = dataset_directory / \"leipzig1M.txt\"\n",
                "textual_dataset = open(textual_dataset_path, \"r\").read().strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "textual_lines = textual_dataset.split(\"\\n\")\n",
                "print(f\"Loaded {len(textual_lines):,} lines of mean length {sum(len(line) for line in textual_lines) / len(textual_lines):.2f} characters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_hashes(line, make_baseline_generator, make_test_generator):\n",
                "    int_hashes = list(make_baseline_generator(line))\n",
                "    float_hashes = list(make_test_generator(line))\n",
                "    if int_hashes != float_hashes:\n",
                "        print(f\"Int Hashes:   {int_hashes}\")\n",
                "        print(f\"Float Hashes: {float_hashes}\")\n",
                "\n",
                "\n",
                "for line in textual_lines[:2]:\n",
                "    compare_hashes(\n",
                "        line,\n",
                "        lambda l: rabin_karp_ints(l, 17, 31, 65521),\n",
                "        lambda l: rabin_karp_floats(l, 17, 31, 65521),\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A bigger question now is, will the same hold, if we use much larger modulo values?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LARGEST_MODULO_SAFE_MODULO = 4503599626977\n",
                "\n",
                "for window_width in [3, 17, 64]:\n",
                "    for line in textual_lines[:50]:\n",
                "        compare_hashes(\n",
                "            line,\n",
                "            lambda l: rabin_karp_ints(l, window_width=window_width, multiplier=257, modulo=LARGEST_MODULO_SAFE_MODULO),\n",
                "            lambda l: rabin_karp_floats(l, window_width=window_width, multiplier=257, modulo=LARGEST_MODULO_SAFE_MODULO))\n",
                "    print(f\"Passed for window width: {window_width}!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Rabin-Karp Rolling Hashing via FMAs\n",
                "\n",
                "- How aggressively can we use **FMA** (Fused Multiply-Add) operations to optimize the algorithm?\n",
                "- How many of the modulo operations can we avoid?\n",
                "- How can we simplify the `%` modulo operation?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "from typing import Generator\n",
                "\n",
                "LARGEST_INTEGRAL_FLOAT: float = 4503599627370495.0\n",
                "\n",
                "\n",
                "def rabin_karp_fma(\n",
                "    s: str,\n",
                "    window_width: int,\n",
                "    multiplier: int,\n",
                "    modulo: int,\n",
                "    alphabet_size: int = 256,\n",
                ") -> Generator[int, None, None]:\n",
                "    \"\"\"Return the rolling polynomial hashes of every length-`window_width` substring of `s`\n",
                "    using Fused-Multiply-Add (FMA) operations & Barrett reduction for performance.\"\"\"\n",
                "\n",
                "    assert window_width > 0, \"Window width must be positive\"\n",
                "    assert multiplier > 0, \"Multiplier must be positive\"\n",
                "    assert modulo > 0, \"Modulo must be positive\"\n",
                "    assert multiplier < modulo, \"Multiplier must be less than modulo\"\n",
                "\n",
                "    if len(s) < window_width:\n",
                "        return\n",
                "\n",
                "    multiplier = float(multiplier)\n",
                "    modulo = float(modulo)\n",
                "    assert (\n",
                "        modulo < LARGEST_INTEGRAL_FLOAT\n",
                "    ), \"Modulo can't exceed the largest integral float value\"\n",
                "\n",
                "    # Ensure, we won't overflow the floating-point representation\n",
                "    largest_post_modulo = modulo - 1\n",
                "    max_possible_term = alphabet_size\n",
                "    assert (\n",
                "        largest_post_modulo * multiplier + max_possible_term <= LARGEST_INTEGRAL_FLOAT\n",
                "    ), \"Will overflow\"\n",
                "\n",
                "    inverse_modulo: float = 1.0 / modulo\n",
                "\n",
                "    # Barrett reduction function\n",
                "    # It will be used to reduce the intermediate results to the modulo range\n",
                "    def barrett_mod(x: float) -> float:\n",
                "        q = math.floor(x * inverse_modulo)\n",
                "        result = x - q * modulo\n",
                "        # Handle potential off-by-one errors\n",
                "        if result >= modulo:\n",
                "            result -= modulo\n",
                "        elif result < 0:\n",
                "            result += modulo\n",
                "        assert result == (x % modulo), \"Barrett reduction failed\"\n",
                "        return result\n",
                "\n",
                "    # All of the operations will happen with a modulo:\n",
                "    def fma_mod(a: float, b: float, c: float) -> float:\n",
                "        intermediate = a * b + c\n",
                "        assert intermediate <= LARGEST_INTEGRAL_FLOAT, \"FMA did exceed integral range\"\n",
                "        return barrett_mod(intermediate)\n",
                "\n",
                "    # Precompute the discarding multiplier\n",
                "    negative_discarding_multiplier: float = 1.0\n",
                "    for _ in range(window_width - 1):\n",
                "        negative_discarding_multiplier = fma_mod(\n",
                "            negative_discarding_multiplier, multiplier, 0.0\n",
                "        )\n",
                "    negative_discarding_multiplier = (\n",
                "        -negative_discarding_multiplier\n",
                "    )  # Negate for FMA compatibility\n",
                "\n",
                "    # Handle the first window - without dropping any characters\n",
                "    current_hash: float = 0.0\n",
                "    for char in s[:window_width]:\n",
                "        new_term = float(ord(char) + 1)\n",
                "        assert new_term <= alphabet_size, \"Pass correct `alphabet_size`\"\n",
                "        current_hash = fma_mod(current_hash, multiplier, new_term)\n",
                "    yield int(current_hash)\n",
                "\n",
                "    # Roll through the rest of the string\n",
                "    total_hashes = len(s) - window_width + 1\n",
                "    for i in range(1, total_hashes):  # First hash is already yielded\n",
                "        old_term = float(ord(s[i - 1]) + 1)\n",
                "        new_term = float(ord(s[i + window_width - 1]) + 1)\n",
                "\n",
                "        # Remove leftmost char and add the new rightmost one.\n",
                "        current_hash = fma_mod(old_term, negative_discarding_multiplier, current_hash)\n",
                "        assert (\n",
                "            current_hash >= -modulo\n",
                "        ), \"Intermediate hash may be negative, but within modulo range\"\n",
                "        current_hash = fma_mod(current_hash, multiplier, new_term)\n",
                "        assert current_hash >= 0, \"Current hash should not be negative\"\n",
                "        yield int(current_hash)\n",
                "\n",
                "\n",
                "# Quick sanity-check\n",
                "assert list(rabin_karp_fma(\"abcd\", 3, 31, 1_000_000_007)) == [\n",
                "    next(rabin_karp_fma(\"abc\", 3, 31, 1_000_000_007)),\n",
                "    next(rabin_karp_fma(\"bcd\", 3, 31, 1_000_000_007)),\n",
                "]\n",
                "assert list(rabin_karp_fma(\"abcdefdhijklmnopqr\", 17, 31, 65521)) == [\n",
                "    next(rabin_karp_fma(\"abcdefdhijklmnopq\", 17, 31, 65521)),\n",
                "    next(rabin_karp_fma(\"bcdefdhijklmnopqr\", 17, 31, 65521)),\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LARGEST_MODULO_SAFE_MODULO = 4503599626977\n",
                "\n",
                "for window_width in [3, 17, 64]:\n",
                "    for line in textual_lines[:50]:\n",
                "        compare_hashes(\n",
                "            line,\n",
                "            lambda l: rabin_karp_ints(l, window_width=window_width, multiplier=257, modulo=LARGEST_MODULO_SAFE_MODULO),\n",
                "            lambda l: rabin_karp_fma(l, window_width=window_width, multiplier=257, modulo=LARGEST_MODULO_SAFE_MODULO))\n",
                "    print(f\"Passed for window width: {window_width}!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we can handle typical texts, let's try several tricky inputs... where we'll be at a brink of an overflow! Some uncomfortable character values are: `\\x00`, `\\x01`, `\\x7F`, `\\xFF`. To really stress-test, let's pick the largest prime number below `LARGEST_INTEGRAL_FLOAT`, that can be used safely for a given alphabet size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Final, List\n",
                "\n",
                "# Fixed witnesses that make Miller-Rabin exact for n < 2**64\n",
                "MR_BASES: Final[List[int]] = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37]\n",
                "\n",
                "\n",
                "def _is_prime_64(n: int) -> bool:\n",
                "    \"\"\"Exact primality for 0 < n < 2**64.\"\"\"\n",
                "    if n < 2:\n",
                "        return False\n",
                "    # Quick reject: small prime factors\n",
                "    for p in MR_BASES:  # covers all primes ≤ 37\n",
                "        if n == p:\n",
                "            return True\n",
                "        if n % p == 0:\n",
                "            return False\n",
                "\n",
                "    # Write n-1 = d · 2ˢ  with d odd\n",
                "    d, s = n - 1, 0\n",
                "    while d & 1 == 0:\n",
                "        d >>= 1\n",
                "        s += 1\n",
                "\n",
                "    # Strong-probable-prime test for each base\n",
                "    for a in MR_BASES:\n",
                "        x = pow(a, d, n)\n",
                "        if x in (1, n - 1):  # self-loop or −1 ⇒ may be prime\n",
                "            continue\n",
                "        for _ in range(s - 1):  # square until −1 or cycle\n",
                "            x = pow(x, 2, n)\n",
                "            if x == n - 1:\n",
                "                break\n",
                "        else:  # never hit −1 ⇒ composite\n",
                "            return False\n",
                "    return True\n",
                "\n",
                "\n",
                "def largest_prime_below(n: int) -> int:\n",
                "    \"\"\"\n",
                "    Return the largest prime strictly less than n (n must be > 2).\n",
                "    Average cost: O(log n * log log n) because the prime gap ~ log n.\n",
                "    \"\"\"\n",
                "    if n <= 2:\n",
                "        raise ValueError(\"Threshold must exceed 2.\")\n",
                "    n -= n % 2 == 0  # make n odd\n",
                "    while not _is_prime_64(n):\n",
                "        n -= 2\n",
                "    return n\n",
                "\n",
                "\n",
                "LARGEST_INTEGRAL_FLOAT_PRIME = largest_prime_below(int(LARGEST_INTEGRAL_FLOAT))\n",
                "print(f\"{LARGEST_INTEGRAL_FLOAT_PRIME:,}\")  # This will be used for stress-testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "\n",
                "all_0 = \"\\x00\" * 1_000\n",
                "all_1 = \"\\x01\" * 1_000\n",
                "all_127 = \"\\x7f\" * 1_000\n",
                "all_255 = \"\\xff\" * 1_000\n",
                "all_0_255 = \"\\x00\\xff\" * 500  # alternating 0 and 255 characters\n",
                "all_uncomfortable = \"\\x00\\x01\\x7f\\xfe\\xff\" * 250  # all uncomfortable characters\n",
                "\n",
                "long_random_strings = [\n",
                "    \"\".join(random.choices(\"\\x00\\x01\\x7f\\xfe\\xff\", k=10_000)) for _ in range(10)\n",
                "]  # 10 long random strings with uncomfortable characters\n",
                "\n",
                "alphabet_size = 256\n",
                "multiplier = 257\n",
                "largest_term = alphabet_size + 1  # in this specific case, same as `multiplier`\n",
                "large_modulo = largest_prime_below(\n",
                "    int(LARGEST_INTEGRAL_FLOAT) // multiplier - largest_term\n",
                ")\n",
                "\n",
                "for window_width in [3, 17, 64, 707]:\n",
                "    for line in [\n",
                "        all_0,\n",
                "        all_1,\n",
                "        all_127,\n",
                "        all_255,\n",
                "        all_0_255,\n",
                "        all_uncomfortable,\n",
                "        *long_random_strings,\n",
                "    ]:\n",
                "        compare_hashes(\n",
                "            line,\n",
                "            lambda l: rabin_karp_ints(\n",
                "                l,\n",
                "                window_width=window_width,\n",
                "                multiplier=multiplier,\n",
                "                modulo=large_modulo,\n",
                "                alphabet_size=alphabet_size,\n",
                "            ),\n",
                "            lambda l: rabin_karp_fma(\n",
                "                l,\n",
                "                window_width=window_width,\n",
                "                multiplier=multiplier,\n",
                "                modulo=large_modulo,\n",
                "                alphabet_size=alphabet_size,\n",
                "            ),\n",
                "        )\n",
                "    print(f\"Passed for window width: {window_width}, modulo: {large_modulo:,}!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Min-Hash Fingerprinting\n",
                "\n",
                "Min-Hash fingerprints transform variable length text representations into **fixed-length vectors**, where each dimension stores the minimum hash value of a certain hash function across the whole document.\n",
                "It's great for large-scale information retrieval using Hamming Distance or Jaccard Similarity ($|A ∩ B| / |A ∪ B|$) or its weighted alternative.\n",
                "\n",
                "A potentially more informative alternative is \"weighted Min-Hash\", which takes into account the frequency of each element in the document. This makes the fingerprints compatible with **TF-IDF**-like algorithms, and makes the system more robust especially for narrow rolling windows."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install tqdm numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from numpy.dtypes import StringDType\n",
                "from typing import List, Tuple\n",
                "\n",
                "def count_min_sketch(\n",
                "    text: str,\n",
                "    window_widths: List[int],\n",
                "    multipliers: List[int],\n",
                "    modulo: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
                "    \"\"\"\n",
                "    Produces a weighted Min-Hash fingerprint also called a Count-Min Sketch.\n",
                "    Those sketches are trivial to merge\n",
                "    \n",
                "    https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch\n",
                "    \"\"\"\n",
                "    \n",
                "    count_widths = len(window_widths)\n",
                "    count_multipliers = len(multipliers)\n",
                "    assert count_widths == count_multipliers, f\"{count_widths=} != {count_multipliers=}\"\n",
                "    \n",
                "    fingerprint_hashes = np.empty((len(window_widths),), dtype=np.uint32)\n",
                "    fingerprint_weights = np.empty((len(window_widths),), dtype=np.uint32)\n",
                "    fingerprint_ngrams = np.empty((len(window_widths),), dtype=StringDType())\n",
                "    \n",
                "    skipped_u32_hash = np.iinfo(np.uint32).max\n",
                "    skipped_u64_intermediary = np.iinfo(np.uint64).max\n",
                "    hashers = [\n",
                "        rabin_karp_fma(text, window_width=width, multiplier=multiplier, modulo=modulo)\n",
                "        for width, multiplier in zip(window_widths, multipliers)\n",
                "    ]\n",
                "    \n",
                "    for i, hasher in enumerate(hashers):\n",
                "        smallest_hash = skipped_u64_intermediary\n",
                "        smallest_count = 0\n",
                "        smallest_example = None\n",
                "        for rolling_intermediate_u64_hash in hasher:\n",
                "            new_smallest_hash = min(smallest_hash, rolling_intermediate_u64_hash)\n",
                "            if new_smallest_hash < smallest_hash:\n",
                "                smallest_count = 1\n",
                "                smallest_hash = new_smallest_hash\n",
                "                smallest_example = text[i:i + window_widths[i]]\n",
                "            elif new_smallest_hash == smallest_hash:\n",
                "                smallest_count += 1\n",
                "            \n",
                "        smallest_hash &= skipped_u32_hash  # Ensure we don't exceed the `uint32` range\n",
                "        fingerprint_hashes[i] = smallest_hash\n",
                "        fingerprint_weights[i] = smallest_count\n",
                "        fingerprint_ngrams[i] = smallest_example\n",
                "\n",
                "    return fingerprint_hashes, fingerprint_weights, fingerprint_ngrams\n",
                "\n",
                "count_min_sketch(\"abcde\", [3, 4], [257, 258], 4503599626977)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A good set of hyper-parameters for Min-Hashing binary text would be:\n",
                "\n",
                "- `window_widths`: ${3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 18, 21, 24, 27, 30}$ - 16 widths\n",
                "- `alphabet_size`: $256$ for ASCII & binary UTF-8 content\n",
                "- `ndim`: $16...1024$, something like 192 should be great for X/Twitter\n",
                "- `multipliers`: ${257, 258, 259, 260, 261, 262, ..., 1024 + 256}$\n",
                "\n",
                "When processing less usual inputs, like the DNA sequences, parameters may be different, e.g.:\n",
                "\n",
                "- `window_widths`: ${3, 6, 9, 12, 15, 30, 60, 120}$\n",
                "- `alphabet_size`: $4$ for DNA sequences\n",
                "- `ndim`: should be probably proportional to $√n$, where $n$ is the typical length of sequences\n",
                "- `multipliers`: ${5, 6, 7, 8, 9, ..., 4 * n + 1}$\n",
                "\n",
                "In every case, the `modulo` should be co-prime to the multiplier.\n",
                "The easiest option is to use a large prime, that can be obtained via:\n",
                "\n",
                "```python\n",
                "largest_prime_below(int(LARGEST_INTEGRAL_FLOAT) // max(multipliers) - (alphabet_size + 1))\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from typing import Tuple\n",
                "\n",
                "\n",
                "def jaccard_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
                "    if a.shape != b.shape:\n",
                "        raise ValueError(\"Fingerprints must have identical length\")\n",
                "\n",
                "    return float(np.mean(a == b))\n",
                "\n",
                "\n",
                "def weighted_jaccard_similarity(\n",
                "    a: Tuple[np.ndarray, np.ndarray],\n",
                "    b: Tuple[np.ndarray, np.ndarray],\n",
                ") -> float:\n",
                "    hashes_a, weights_a = a\n",
                "    hashes_b, weights_b = b\n",
                "\n",
                "    if hashes_a.shape != hashes_b.shape or weights_a.shape != weights_b.shape:\n",
                "        raise ValueError(\"Both fingerprints must have identical dimensions\")\n",
                "\n",
                "    magnitude_i = (weights_a * weights_b)[hashes_a == hashes_b].sum()\n",
                "    magnitude_a = (weights_a * weights_a).sum()\n",
                "    magnitude_b = (weights_b * weights_b).sum()\n",
                "    magnitude_u = magnitude_a + magnitude_b - magnitude_i\n",
                "\n",
                "    return float(magnitude_i) / float(magnitude_u)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's compute the rolling fingerprints:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "textual_dataset_path = dataset_directory / \"leipzig1M.txt\"\n",
                "textual_dataset = open(textual_dataset_path, \"r\").read().strip()\n",
                "textual_lines = textual_dataset.split(\"\\n\")\n",
                "print(f\"Loaded {len(textual_lines):,} lines of mean length {sum(len(line) for line in textual_lines) / len(textual_lines):.2f} characters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "multipliers = list(range(256, 256+192))\n",
                "window_widths = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 18, 21, 24, 27, 30]\n",
                "window_widths *= (192 // len(window_widths))\n",
                "LARGEST_MODULO_SAFE_MODULO = 4503599626977\n",
                "\n",
                "fingerprint_hashes = []\n",
                "fingerprint_counts = []\n",
                "fingerprint_ngrams = []\n",
                "\n",
                "DATASET_SIZE_LIMIT = 10_000\n",
                "\n",
                "for line in tqdm(textual_lines[:DATASET_SIZE_LIMIT], desc=\"Fingerprinting lines\", unit=\"line\"):\n",
                "    hashes, counts, ngrams = count_min_sketch(\n",
                "        text=line,\n",
                "        window_widths=window_widths,\n",
                "        multipliers=multipliers,\n",
                "        modulo= LARGEST_MODULO_SAFE_MODULO,\n",
                "    )\n",
                "    fingerprint_hashes.append(hashes)\n",
                "    fingerprint_counts.append(counts)\n",
                "    fingerprint_ngrams.append(ngrams)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's estimate Recall @ 1, but before we do that - let's find a way to highlight N-gram matches between strings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "COLOR_ARRAY = [\n",
                "    \"\\033[38;5;196m\",  # red\n",
                "    \"\\033[38;5;208m\",  # orange\n",
                "    \"\\033[38;5;226m\",  # yellow\n",
                "    \"\\033[38;5;082m\",  # green\n",
                "    \"\\033[38;5;039m\",  # blue\n",
                "    \"\\033[38;5;201m\",  # magenta\n",
                "    \"\\033[38;5;129m\",  # purple\n",
                "]\n",
                "\n",
                "# Sometimes hashes match, but the n-grams are different\n",
                "COLOR_COLLISION = \"\\033[38;5;244m\"  # grey\n",
                "COLOR_RESET = \"\\033[0m\"\n",
                "\n",
                "def color_code_matches(\n",
                "    query_text: str,\n",
                "    document_text: str,\n",
                "    query_hashes: np.ndarray,\n",
                "    document_hashes: np.ndarray,\n",
                "    query_ngrams: np.ndarray,\n",
                "    document_ngrams: np.ndarray,\n",
                ") -> Tuple[str, str]:\n",
                "    \n",
                "    color_index = 0\n",
                "    for dim in range(len(query_hashes)):\n",
                "        is_matching_hash = query_hashes[dim] == document_hashes[dim]\n",
                "        is_matching_ngram = query_ngrams[dim] == document_ngrams[dim]\n",
                "        \n",
                "        if is_matching_ngram:\n",
                "            color = COLOR_ARRAY[color_index % len(COLOR_ARRAY)]\n",
                "            ngram_replacement = f\"{color}{query_ngrams[dim]}{COLOR_RESET}\"\n",
                "            query_text = query_text.replace(query_ngrams[dim], ngram_replacement)\n",
                "            document_text = document_text.replace(document_ngrams[dim], ngram_replacement)\n",
                "            color_index += 1\n",
                "        elif is_matching_hash:\n",
                "            ngram_replacement = f\"{COLOR_COLLISION}{query_ngrams[dim]}{COLOR_RESET}\"\n",
                "            query_text = query_text.replace(query_ngrams[dim], ngram_replacement)\n",
                "            document_text = document_text.replace(document_ngrams[dim], ngram_replacement)\n",
                "    \n",
                "    return query_text, document_text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "QUERIES_TO_COMPARE = 100\n",
                "\n",
                "for i, query_hashes, query_counts, query_ngrams in tqdm(zip(\n",
                "    range(QUERIES_TO_COMPARE),\n",
                "    fingerprint_hashes[:QUERIES_TO_COMPARE],\n",
                "    fingerprint_counts[:QUERIES_TO_COMPARE],\n",
                "    fingerprint_ngrams[:QUERIES_TO_COMPARE],\n",
                "), desc=\"Searching\", unit=\"doc\", total=QUERIES_TO_COMPARE):\n",
                "    \n",
                "    # Compare with all other fingerprints\n",
                "    best_score, best_index = 0.0, -1\n",
                "    for j, dataset_hashes, dataset_counts, dataset_ngrams in zip(\n",
                "        range(len(fingerprint_hashes)),\n",
                "        fingerprint_hashes,\n",
                "        fingerprint_counts,\n",
                "        fingerprint_ngrams,\n",
                "    ):\n",
                "        if i == j:\n",
                "            continue\n",
                "\n",
                "        score = jaccard_similarity(query_hashes, dataset_hashes)\n",
                "        if score > best_score:\n",
                "            best_score = score\n",
                "            best_index = j\n",
                "\n",
                "    query = textual_lines[i]\n",
                "    doc = textual_lines[best_index]\n",
                "    colored_query, colored_doc = color_code_matches(\n",
                "        query_text=query,\n",
                "        document_text=doc,\n",
                "        query_hashes=query_hashes,\n",
                "        document_hashes=fingerprint_hashes[best_index],\n",
                "        query_ngrams=query_ngrams,\n",
                "        document_ngrams=fingerprint_ngrams[best_index],\n",
                "    )\n",
                "    print(f\"Matched query {i:,} with document {best_index:,} with score {best_score:.4f}\")\n",
                "    print(f\"- {colored_query}\")\n",
                "    print(f\"- {colored_doc}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "QUERIES_TO_COMPARE = 100\n",
                "\n",
                "for i, query_hashes, query_counts, query_ngrams in tqdm(\n",
                "    zip(\n",
                "        range(QUERIES_TO_COMPARE),\n",
                "        fingerprint_hashes[:QUERIES_TO_COMPARE],\n",
                "        fingerprint_counts[:QUERIES_TO_COMPARE],\n",
                "        fingerprint_ngrams[:QUERIES_TO_COMPARE],\n",
                "    ),\n",
                "    desc=\"Searching\",\n",
                "    unit=\"doc\",\n",
                "    total=QUERIES_TO_COMPARE,\n",
                "):\n",
                "\n",
                "    # Compare with all other fingerprints\n",
                "    best_score, best_index = 0.0, -1\n",
                "    for j, dataset_hashes, dataset_counts, dataset_ngrams in zip(\n",
                "        range(len(fingerprint_hashes)),\n",
                "        fingerprint_hashes,\n",
                "        fingerprint_counts,\n",
                "        fingerprint_ngrams,\n",
                "    ):\n",
                "        if i == j:\n",
                "            continue\n",
                "\n",
                "        score = weighted_jaccard_similarity(\n",
                "            query_hashes,\n",
                "            dataset_hashes,\n",
                "            query_ngrams,\n",
                "            dataset_ngrams,\n",
                "        )\n",
                "        if score > best_score:\n",
                "            best_score = score\n",
                "            best_index = j\n",
                "\n",
                "    query = textual_lines[i]\n",
                "    doc = textual_lines[best_index]\n",
                "    colored_query, colored_doc = color_code_matches(\n",
                "        query_text=query,\n",
                "        document_text=doc,\n",
                "        query_hashes=query_hashes,\n",
                "        document_hashes=fingerprint_hashes[best_index],\n",
                "        query_ngrams=query_ngrams,\n",
                "        document_ngrams=fingerprint_ngrams[best_index],\n",
                "    )\n",
                "    print(\n",
                "        f\"Matched query {i:,} with document {best_index:,} with score {best_score:.4f}\"\n",
                "    )\n",
                "    print(f\"- {colored_query}\")\n",
                "    print(f\"- {colored_doc}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Min-Hash Fingerprinting DNA & Protein Sequences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dna_dataset_path = dataset_directory / \"acgt_10k.txt\"\n",
                "dna_dataset = open(dna_dataset_path, \"r\").read().strip()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "StringZilla",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
