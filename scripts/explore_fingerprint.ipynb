{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exploring Rabin-Karp-style Min-Hash Fingerprinting\n",
                "\n",
                "This document showcases the differences between different numeric types that one can use to implement a Rabin-Karp-style min-hash fingerprinting algorithm.\n",
                "It answers several important questions:\n",
                "\n",
                "- How to use floating-point numbers for a traditionally integer-based task - \"hashing\"?\n",
                "- How to properly compose many such hash functions to maximize the quality of fingerprints?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Rabin-Karp Rolling Hashing\n",
                "\n",
                "Rabin-Karp algorithm is a polynomial rolling hash function built around modulo arithmetic.\n",
                "Once the hashing window rolls forward, the leftmost character is removed and a new rightmost character is added.\n",
                "Thus, the cost of computing each slices hash is just $O(1)$, if the previous window's hash is known.\n",
                "\n",
                "Assuming, many such rolling hashes will be used later, we can parameterize the algorithm with a few parameters:\n",
                "- `window_width` - the length of the substring to hash;\n",
                "- `multiplier` - the multiplier for the polynomial hash;\n",
                "- `modulo` - the modulo to use for the hash, generally prime;\n",
                "- `alphabet_size` - the size of the alphabet used in the string, e.g. 256 for ASCII;\n",
                "- `salt` - an optional salt to add to each character's ordinal value, usually 1 to avoid adding zeroes;\n",
                "- `seed` - an optional seed for the first hash, can be 0."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Generator\n",
                "\n",
                "\n",
                "def rabin_karp_ints(\n",
                "    s: str,\n",
                "    window_width: int,\n",
                "    multiplier: int,\n",
                "    modulo: int,\n",
                "    alphabet_size: int = 256,\n",
                "    salt: int = 1,\n",
                ") -> Generator[int, None, None]:\n",
                "    \"\"\"Return the rolling polynomial hashes of every length-`window_width` substring of `s`\"\"\"\n",
                "\n",
                "    assert window_width > 0, \"Window width must be positive\"\n",
                "    assert multiplier > 0, \"Multiplier must be positive\"\n",
                "    assert modulo > 0, \"Modulo must be positive\"\n",
                "    assert multiplier < modulo, \"Multiplier must be less than modulo\"\n",
                "\n",
                "    if len(s) < window_width:\n",
                "        return\n",
                "\n",
                "    current_hash: int = 0\n",
                "    for char in s[:window_width]:\n",
                "        new_term = ord(char) + salt\n",
                "        assert new_term < (alphabet_size + salt), \"Pass correct `alphabet_size`\"\n",
                "        current_hash = (current_hash * multiplier + new_term) % modulo\n",
                "    yield current_hash\n",
                "\n",
                "    discarding_multiplier: int = pow(multiplier, window_width - 1, modulo)\n",
                "    total_hashes = len(s) - window_width + 1\n",
                "    for i in range(1, total_hashes):  # First hash is already yielded\n",
                "        old_term = ord(s[i - 1]) + salt\n",
                "        new_term = ord(s[i + window_width - 1]) + salt\n",
                "\n",
                "        # Remove leftmost char and add the new rightmost one.\n",
                "        # All operations must be modulo `modulo`, but assuming the infinite precision of integers,\n",
                "        # we don't care in this draft.\n",
                "        current_hash = (current_hash - old_term * discarding_multiplier) % modulo\n",
                "        current_hash = (current_hash * multiplier + new_term) % modulo\n",
                "        yield current_hash\n",
                "\n",
                "\n",
                "# Quick sanity-check\n",
                "assert list(rabin_karp_ints(\"abcd\", 3, 31, 1_000_000_007)) == [\n",
                "    next(rabin_karp_ints(\"abc\", 3, 31, 1_000_000_007)),\n",
                "    next(rabin_karp_ints(\"bcd\", 3, 31, 1_000_000_007)),\n",
                "]\n",
                "assert list(rabin_karp_ints(\"abcdefdhijklmnopqr\", 17, 31, 65521)) == [\n",
                "    next(rabin_karp_ints(\"abcdefdhijklmnopq\", 17, 31, 65521)),\n",
                "    next(rabin_karp_ints(\"bcdefdhijklmnopqr\", 17, 31, 65521)),\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Rabin-Karp Rolling Hashing via Floats\n",
                "\n",
                "The Python's `int` type is unbounded, so it can be used to implement the Rabin-Karp rolling hash algorithm without worrying about overflow.\n",
                "It is, however, insanely expensive to use, and doesn't allow us to explore optimization opportunities.\n",
                "The `float`, on the other hand, is just a double-precision IEEE 754 floating-point number, which can exactly represent 52-bit integers!\n",
                "Thus, we can convert our arithmetic to use `float`s, if we guarantee, that no intermediate result will exceed that limit."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Generator\n",
                "\n",
                "LARGEST_INTEGRAL_FLOAT: float = 4503599627370495.0\n",
                "\n",
                "\n",
                "def rabin_karp_floats(\n",
                "    s: str,\n",
                "    window_width: int,\n",
                "    multiplier: int,\n",
                "    modulo: int,\n",
                "    alphabet_size: int = 256,\n",
                "    salt: int = 1,\n",
                ") -> Generator[int, None, None]:\n",
                "    \"\"\"Return the rolling polynomial hashes of every length-`window_width` substring of `s`\"\"\"\n",
                "\n",
                "    assert window_width > 0, \"Window width must be positive\"\n",
                "    assert multiplier > 0, \"Multiplier must be positive\"\n",
                "    assert modulo > 0, \"Modulo must be positive\"\n",
                "    assert multiplier < modulo, \"Multiplier must be less than modulo\"\n",
                "\n",
                "    if len(s) < window_width:\n",
                "        return\n",
                "\n",
                "    multiplier = float(multiplier)\n",
                "    modulo = float(modulo)\n",
                "    assert (\n",
                "        modulo < LARGEST_INTEGRAL_FLOAT\n",
                "    ), \"Modulo can't exceed the largest integral float value\"\n",
                "\n",
                "    # Ensure, we won't overflow the floating-point representation\n",
                "    largest_post_modulo = modulo - 1\n",
                "    max_possible_term = alphabet_size\n",
                "    assert (\n",
                "        largest_post_modulo * multiplier + max_possible_term <= LARGEST_INTEGRAL_FLOAT\n",
                "    ), \"Will overflow\"\n",
                "\n",
                "    # All of the operations will happen with a modulo:\n",
                "    def mul_mod(a: float, b: float) -> float:\n",
                "        return (a * b) % modulo\n",
                "\n",
                "    def add_mod(a: float, b: float) -> float:\n",
                "        return (a + b) % modulo\n",
                "\n",
                "    def sub_mod(a: float, b: float) -> float:\n",
                "        return (a - b) % modulo\n",
                "\n",
                "    # Precompute the discarding multiplier\n",
                "    discarding_multiplier: float = 1.0\n",
                "    for _ in range(window_width - 1):\n",
                "        discarding_multiplier = mul_mod(discarding_multiplier, multiplier)\n",
                "\n",
                "    # Handle the first window - without dropping any characters\n",
                "    current_hash: float = 0.0\n",
                "    for char in s[:window_width]:\n",
                "        new_term = float(ord(char) + salt)\n",
                "        assert new_term < (alphabet_size + salt), \"Pass correct `alphabet_size`\"\n",
                "        current_hash = add_mod(mul_mod(current_hash, multiplier), new_term)\n",
                "    yield int(current_hash)\n",
                "\n",
                "    # Roll through the rest of the string\n",
                "    total_hashes = len(s) - window_width + 1\n",
                "    for i in range(1, total_hashes):  # First hash is already yielded\n",
                "        old_term = float(ord(s[i - 1]) + salt)\n",
                "        new_term = float(ord(s[i + window_width - 1]) + salt)\n",
                "\n",
                "        # Remove leftmost char and add the new rightmost one.\n",
                "        current_hash = sub_mod(current_hash, mul_mod(old_term, discarding_multiplier))\n",
                "        current_hash = add_mod(mul_mod(current_hash, multiplier), new_term)\n",
                "        yield int(current_hash)\n",
                "\n",
                "\n",
                "# Quick sanity-check\n",
                "assert list(rabin_karp_floats(\"abcd\", 3, 31, 1_000_000_007)) == [\n",
                "    next(rabin_karp_floats(\"abc\", 3, 31, 1_000_000_007)),\n",
                "    next(rabin_karp_floats(\"bcd\", 3, 31, 1_000_000_007)),\n",
                "]\n",
                "assert list(rabin_karp_floats(\"abcdefdhijklmnopqr\", 17, 31, 65521)) == [\n",
                "    next(rabin_karp_floats(\"abcdefdhijklmnopq\", 17, 31, 65521)),\n",
                "    next(rabin_karp_floats(\"bcdefdhijklmnopqr\", 17, 31, 65521)),\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's load some data and ensure that the outputs are identical between the `int` and `float` implementations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "dataset_directory = Path(\"..\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "textual_dataset_path = dataset_directory / \"leipzig1M.txt\"\n",
                "textual_dataset = open(textual_dataset_path, \"r\").read().strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 1,000,000 lines of mean length 128.64 characters\n"
                    ]
                }
            ],
            "source": [
                "textual_lines = textual_dataset.split(\"\\n\")\n",
                "print(f\"Loaded {len(textual_lines):,} lines of mean length {sum(len(line) for line in textual_lines) / len(textual_lines):.2f} characters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_hashes(line, make_baseline_generator, make_test_generator):\n",
                "    int_hashes = list(make_baseline_generator(line))\n",
                "    float_hashes = list(make_test_generator(line))\n",
                "    if int_hashes != float_hashes:\n",
                "        print(f\"Int Hashes:   {int_hashes}\")\n",
                "        print(f\"Float Hashes: {float_hashes}\")\n",
                "\n",
                "\n",
                "for line in textual_lines[:2]:\n",
                "    compare_hashes(\n",
                "        line,\n",
                "        lambda l: rabin_karp_ints(l, 17, 31, 65521),\n",
                "        lambda l: rabin_karp_floats(l, 17, 31, 65521),\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A bigger question now is, will the same hold, if we use much larger modulo values?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Passed for window width: 3!\n",
                        "Passed for window width: 17!\n",
                        "Passed for window width: 64!\n"
                    ]
                }
            ],
            "source": [
                "LARGEST_SAFE_MODULO = 4503599626977\n",
                "\n",
                "for window_width in [3, 17, 64]:\n",
                "    for line in textual_lines[:50]:\n",
                "        compare_hashes(\n",
                "            line,\n",
                "            lambda l: rabin_karp_ints(l, window_width=window_width, multiplier=257, modulo=LARGEST_SAFE_MODULO),\n",
                "            lambda l: rabin_karp_floats(l, window_width=window_width, multiplier=257, modulo=LARGEST_SAFE_MODULO))\n",
                "    print(f\"Passed for window width: {window_width}!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Rabin-Karp Rolling Hashing via FMAs\n",
                "\n",
                "- How aggressively can we use **FMA** (Fused Multiply-Add) operations to optimize the algorithm?\n",
                "- How many of the modulo operations can we avoid?\n",
                "- How can we simplify the `%` modulo operation?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "from typing import Generator\n",
                "\n",
                "LARGEST_INTEGRAL_FLOAT: float = 4503599627370495.0\n",
                "\n",
                "\n",
                "def rabin_karp_fma(\n",
                "    s: str,\n",
                "    window_width: int,\n",
                "    multiplier: int,\n",
                "    modulo: int,\n",
                "    alphabet_size: int = 256,\n",
                "    salt: int = 1,\n",
                ") -> Generator[int, None, None]:\n",
                "    \"\"\"Return the rolling polynomial hashes of every length-`window_width` substring of `s`\n",
                "    using Fused-Multiply-Add (FMA) operations & Barrett reduction for performance.\"\"\"\n",
                "\n",
                "    assert window_width > 0, \"Window width must be positive\"\n",
                "    assert multiplier > 0, \"Multiplier must be positive\"\n",
                "    assert modulo > 0, \"Modulo must be positive\"\n",
                "    assert multiplier < modulo, \"Multiplier must be less than modulo\"\n",
                "\n",
                "    if len(s) < window_width:\n",
                "        return\n",
                "\n",
                "    multiplier = float(multiplier)\n",
                "    modulo = float(modulo)\n",
                "    assert (\n",
                "        modulo < LARGEST_INTEGRAL_FLOAT\n",
                "    ), \"Modulo can't exceed the largest integral float value\"\n",
                "\n",
                "    # Ensure, we won't overflow the floating-point representation\n",
                "    largest_post_modulo = modulo - 1\n",
                "    max_possible_term = alphabet_size\n",
                "    assert (\n",
                "        largest_post_modulo * multiplier + max_possible_term <= LARGEST_INTEGRAL_FLOAT\n",
                "    ), \"Will overflow\"\n",
                "\n",
                "    inverse_modulo: float = 1.0 / modulo\n",
                "\n",
                "    # Barrett reduction function\n",
                "    # It will be used to reduce the intermediate results to the modulo range\n",
                "    def barrett_mod(x: float) -> float:\n",
                "        q = math.floor(x * inverse_modulo)\n",
                "        result = x - q * modulo\n",
                "        # Handle potential off-by-one errors\n",
                "        if result >= modulo:\n",
                "            result -= modulo\n",
                "        elif result < 0:\n",
                "            result += modulo\n",
                "        assert result == (x % modulo), \"Barrett reduction failed\"\n",
                "        return result\n",
                "\n",
                "    # All of the operations will happen with a modulo:\n",
                "    def fma_mod(a: float, b: float, c: float) -> float:\n",
                "        intermediate = a * b + c\n",
                "        assert intermediate <= LARGEST_INTEGRAL_FLOAT, \"FMA did exceed integral range\"\n",
                "        return barrett_mod(intermediate)\n",
                "\n",
                "    # Precompute the discarding multiplier\n",
                "    negative_discarding_multiplier: float = 1.0\n",
                "    for _ in range(window_width - 1):\n",
                "        negative_discarding_multiplier = fma_mod(\n",
                "            negative_discarding_multiplier, multiplier, 0.0\n",
                "        )\n",
                "    negative_discarding_multiplier = (\n",
                "        -negative_discarding_multiplier\n",
                "    )  # Negate for FMA compatibility\n",
                "\n",
                "    # Handle the first window - without dropping any characters\n",
                "    current_hash: float = 0.0\n",
                "    for char in s[:window_width]:\n",
                "        new_term = float(ord(char) + salt)\n",
                "        assert new_term < (alphabet_size + salt), \"Pass correct `alphabet_size`\"\n",
                "        current_hash = fma_mod(current_hash, multiplier, new_term)\n",
                "    yield int(current_hash)\n",
                "\n",
                "    # Roll through the rest of the string\n",
                "    total_hashes = len(s) - window_width + 1\n",
                "    for i in range(1, total_hashes):  # First hash is already yielded\n",
                "        old_term = float(ord(s[i - 1]) + salt)\n",
                "        new_term = float(ord(s[i + window_width - 1]) + salt)\n",
                "\n",
                "        # Remove leftmost char and add the new rightmost one.\n",
                "        current_hash = fma_mod(old_term, negative_discarding_multiplier, current_hash)\n",
                "        assert (\n",
                "            current_hash >= -modulo\n",
                "        ), \"Intermediate hash may be negative, but within modulo range\"\n",
                "        current_hash = fma_mod(current_hash, multiplier, new_term)\n",
                "        assert current_hash >= 0, \"Current hash should not be negative\"\n",
                "        yield int(current_hash)\n",
                "\n",
                "\n",
                "# Quick sanity-check\n",
                "assert list(rabin_karp_fma(\"abcd\", 3, 31, 1_000_000_007)) == [\n",
                "    next(rabin_karp_fma(\"abc\", 3, 31, 1_000_000_007)),\n",
                "    next(rabin_karp_fma(\"bcd\", 3, 31, 1_000_000_007)),\n",
                "]\n",
                "assert list(rabin_karp_fma(\"abcdefdhijklmnopqr\", 17, 31, 65521)) == [\n",
                "    next(rabin_karp_fma(\"abcdefdhijklmnopq\", 17, 31, 65521)),\n",
                "    next(rabin_karp_fma(\"bcdefdhijklmnopqr\", 17, 31, 65521)),\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Passed for window width: 3!\n",
                        "Passed for window width: 17!\n",
                        "Passed for window width: 64!\n"
                    ]
                }
            ],
            "source": [
                "LARGEST_SAFE_MODULO = 4503599626977\n",
                "\n",
                "for window_width in [3, 17, 64]:\n",
                "    for line in textual_lines[:50]:\n",
                "        compare_hashes(\n",
                "            line,\n",
                "            lambda l: rabin_karp_ints(l, window_width=window_width, multiplier=257, modulo=LARGEST_SAFE_MODULO),\n",
                "            lambda l: rabin_karp_fma(l, window_width=window_width, multiplier=257, modulo=LARGEST_SAFE_MODULO))\n",
                "    print(f\"Passed for window width: {window_width}!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we can handle typical texts, let's try several tricky inputs... where we'll be at a brink of an overflow! Some uncomfortable character values are: `\\x00`, `\\x01`, `\\x7F`, `\\xFF`. To really stress-test, let's pick the largest prime number below `LARGEST_INTEGRAL_FLOAT`, that can be used safely for a given alphabet size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "4,503,599,627,370,449\n"
                    ]
                }
            ],
            "source": [
                "from typing import Final, List, Generator\n",
                "\n",
                "# Fixed witnesses that make Miller-Rabin exact for n < 2**64\n",
                "MR_BASES: Final[List[int]] = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37]\n",
                "\n",
                "\n",
                "def _is_prime_64(n: int) -> bool:\n",
                "    \"\"\"Exact primality for 0 < n < 2**64.\"\"\"\n",
                "    if n < 2:\n",
                "        return False\n",
                "    # Quick reject: small prime factors\n",
                "    for p in MR_BASES:  # covers all primes ≤ 37\n",
                "        if n == p:\n",
                "            return True\n",
                "        if n % p == 0:\n",
                "            return False\n",
                "\n",
                "    # Write n-1 = d · 2ˢ  with d odd\n",
                "    d, s = n - 1, 0\n",
                "    while d & 1 == 0:\n",
                "        d >>= 1\n",
                "        s += 1\n",
                "\n",
                "    # Strong-probable-prime test for each base\n",
                "    for a in MR_BASES:\n",
                "        x = pow(a, d, n)\n",
                "        if x in (1, n - 1):  # self-loop or −1 ⇒ may be prime\n",
                "            continue\n",
                "        for _ in range(s - 1):  # square until −1 or cycle\n",
                "            x = pow(x, 2, n)\n",
                "            if x == n - 1:\n",
                "                break\n",
                "        else:  # never hit −1 ⇒ composite\n",
                "            return False\n",
                "    return True\n",
                "\n",
                "\n",
                "def prev_primes(n: int) -> Generator[int, None, None]:\n",
                "    \"\"\"\n",
                "    Yield the largest primes strictly less than n (n must be > 2).\n",
                "    Average cost: O(log n * log log n) because the prime gap ~ log n.\n",
                "    \"\"\"\n",
                "    if n <= 2:\n",
                "        raise ValueError(\"Threshold must exceed 2.\")\n",
                "    n -= n % 2 == 0  # make n odd\n",
                "    while n > 2:\n",
                "        if _is_prime_64(n):\n",
                "            yield n\n",
                "        n -= 2\n",
                "\n",
                "def next_primes(n: int) -> Generator[int, None, None]:\n",
                "    \"\"\"\n",
                "    Yield the smallest primes strictly greater than n (n must be > 2).\n",
                "    Average cost: O(log n * log log n) because the prime gap ~ log n.\n",
                "    \"\"\"\n",
                "    if n <= 2:\n",
                "        raise ValueError(\"Threshold must exceed 2.\")\n",
                "    n += n % 2 == 0  # make n odd\n",
                "    while True:\n",
                "        if _is_prime_64(n):\n",
                "            yield n\n",
                "        n += 2\n",
                "\n",
                "LARGEST_INTEGRAL_FLOAT_PRIME = next(prev_primes(int(LARGEST_INTEGRAL_FLOAT)))\n",
                "print(f\"{LARGEST_INTEGRAL_FLOAT_PRIME:,}\")  # This will be used for stress-testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Passed for window width: 3, modulo: 17,523,733,958,369!\n",
                        "Passed for window width: 17, modulo: 17,523,733,958,369!\n",
                        "Passed for window width: 64, modulo: 17,523,733,958,369!\n",
                        "Passed for window width: 707, modulo: 17,523,733,958,369!\n"
                    ]
                }
            ],
            "source": [
                "import random\n",
                "\n",
                "all_0 = \"\\x00\" * 1_000\n",
                "all_1 = \"\\x01\" * 1_000\n",
                "all_127 = \"\\x7f\" * 1_000\n",
                "all_255 = \"\\xff\" * 1_000\n",
                "all_0_255 = \"\\x00\\xff\" * 500  # alternating 0 and 255 characters\n",
                "all_uncomfortable = \"\\x00\\x01\\x7f\\xfe\\xff\" * 250  # all uncomfortable characters\n",
                "\n",
                "long_random_strings = [\n",
                "    \"\".join(random.choices(\"\\x00\\x01\\x7f\\xfe\\xff\", k=10_000)) for _ in range(10)\n",
                "]  # 10 long random strings with uncomfortable characters\n",
                "\n",
                "alphabet_size = 256\n",
                "multiplier = 257\n",
                "largest_term = alphabet_size + 1  # in this specific case, same as `multiplier`\n",
                "large_modulo = next(prev_primes(\n",
                "    int(LARGEST_INTEGRAL_FLOAT) // multiplier - largest_term\n",
                "))\n",
                "\n",
                "for window_width in [3, 17, 64, 707]:\n",
                "    for line in [\n",
                "        all_0,\n",
                "        all_1,\n",
                "        all_127,\n",
                "        all_255,\n",
                "        all_0_255,\n",
                "        all_uncomfortable,\n",
                "        *long_random_strings,\n",
                "    ]:\n",
                "        compare_hashes(\n",
                "            line,\n",
                "            lambda l: rabin_karp_ints(\n",
                "                l,\n",
                "                window_width=window_width,\n",
                "                multiplier=multiplier,\n",
                "                modulo=large_modulo,\n",
                "                alphabet_size=alphabet_size,\n",
                "            ),\n",
                "            lambda l: rabin_karp_fma(\n",
                "                l,\n",
                "                window_width=window_width,\n",
                "                multiplier=multiplier,\n",
                "                modulo=large_modulo,\n",
                "                alphabet_size=alphabet_size,\n",
                "            ),\n",
                "        )\n",
                "    print(f\"Passed for window width: {window_width}, modulo: {large_modulo:,}!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Min-Hash Fingerprinting\n",
                "\n",
                "Min-Hash fingerprints transform variable length text representations into **fixed-length vectors**, where each dimension stores the minimum hash value of a certain hash function across the whole document.\n",
                "It's great for large-scale information retrieval using Hamming Distance or Jaccard Similarity ($|A ∩ B| / |A ∪ B|$) or its weighted alternative.\n",
                "\n",
                "A potentially more informative alternative is \"weighted Min-Hash\", which takes into account the frequency of each element in the document. This makes the fingerprints compatible with **TF-IDF**-like algorithms, and makes the system more robust especially for narrow rolling windows."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (4.67.1)\n",
                        "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (2.2.4)\n"
                    ]
                }
            ],
            "source": [
                "!pip install tqdm numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(14038566040298863954, 12264879942290955073)"
                        ]
                    },
                    "execution_count": 50,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from stringzilla import hash as sz_hash\n",
                "\n",
                "sz_hash(\"abc\", 200), sz_hash(\"abc\", 201)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(array([2256051662, 1712240109], dtype=uint32),\n",
                            " array([3, 2], dtype=uint32),\n",
                            " array(['abc', 'abcd'], dtype=StringDType()))"
                        ]
                    },
                    "execution_count": 51,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import numpy as np\n",
                "from numpy.dtypes import StringDType\n",
                "from typing import List, Tuple\n",
                "\n",
                "\n",
                "def count_min_sketch(\n",
                "    text: str,\n",
                "    window_widths: List[int],\n",
                "    seeds: List[int],\n",
                "    hash_resolution: np.dtype = np.uint32,\n",
                ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
                "    \"\"\"\n",
                "    Produces a weighted Min-Hash fingerprint also called a Count-Min Sketch.\n",
                "    Uses StringZilla's native hash function, as opposed to the Rabin Karp.\n",
                "\n",
                "    https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch\n",
                "    \"\"\"\n",
                "\n",
                "    fingerprint_hashes = np.empty((len(window_widths),), dtype=hash_resolution)\n",
                "    fingerprint_weights = np.empty((len(window_widths),), dtype=np.uint32)\n",
                "    fingerprint_ngrams = np.empty((len(window_widths),), dtype=StringDType())\n",
                "\n",
                "    skipped_final_hash = np.iinfo(hash_resolution).max\n",
                "    skipped_u64_intermediary = np.iinfo(np.uint64).max\n",
                "\n",
                "    for i, (window_width, seed) in enumerate(zip(window_widths, seeds)):\n",
                "        assert window_width > 0, \"Window width must be positive\"\n",
                "\n",
                "        smallest_hash = skipped_u64_intermediary\n",
                "        smallest_count = 0\n",
                "        smallest_example = None\n",
                "\n",
                "        for j in range(len(text) - window_width + 1):\n",
                "            text_window = text[j : j + window_width]\n",
                "            rolling_intermediate_u64_hash = sz_hash(text_window, seed)\n",
                "            new_smallest_hash = min(smallest_hash, rolling_intermediate_u64_hash)\n",
                "            if new_smallest_hash < smallest_hash:\n",
                "                smallest_count = 1\n",
                "                smallest_hash = new_smallest_hash\n",
                "                smallest_example = text_window\n",
                "            elif new_smallest_hash == smallest_hash:\n",
                "                smallest_count += 1\n",
                "\n",
                "        smallest_hash &= skipped_final_hash  # Ensure we don't exceed the `uint32` range\n",
                "        fingerprint_hashes[i] = smallest_hash\n",
                "        fingerprint_weights[i] = smallest_count\n",
                "        fingerprint_ngrams[i] = smallest_example\n",
                "\n",
                "    return fingerprint_hashes, fingerprint_weights, fingerprint_ngrams\n",
                "\n",
                "\n",
                "count_min_sketch(\"abcde\", window_widths=[3, 4], seeds=[257, 258])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(array([   6498345, 1706860248], dtype=uint32),\n",
                            " array([3, 2], dtype=uint32),\n",
                            " array(['abc', 'bcde'], dtype=StringDType()))"
                        ]
                    },
                    "execution_count": 52,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import numpy as np\n",
                "from numpy.dtypes import StringDType\n",
                "from typing import List, Tuple\n",
                "\n",
                "\n",
                "def rolling_count_min_sketch(\n",
                "    text: str,\n",
                "    window_widths: List[int],\n",
                "    multipliers: List[int],\n",
                "    salts: List[int],\n",
                "    modulo: int,\n",
                "    hash_resolution: np.dtype = np.uint32,\n",
                ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
                "    \"\"\"\n",
                "    Produces a weighted Min-Hash fingerprint also called a Count-Min Sketch.\n",
                "    Those sketches are trivial to merge\n",
                "\n",
                "    https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch\n",
                "    \"\"\"\n",
                "\n",
                "    count_widths = len(window_widths)\n",
                "    count_multipliers = len(multipliers)\n",
                "    assert count_widths == count_multipliers, f\"{count_widths=} != {count_multipliers=}\"\n",
                "\n",
                "    fingerprint_hashes = np.empty((len(window_widths),), dtype=hash_resolution)\n",
                "    fingerprint_weights = np.empty((len(window_widths),), dtype=np.uint32)\n",
                "    fingerprint_ngrams = np.empty((len(window_widths),), dtype=StringDType())\n",
                "\n",
                "    skipped_final_hash = np.iinfo(hash_resolution).max\n",
                "    skipped_u64_intermediary = np.iinfo(np.uint64).max\n",
                "    hashers = [\n",
                "        rabin_karp_fma(\n",
                "            text,\n",
                "            window_width=width,\n",
                "            multiplier=multiplier,\n",
                "            modulo=modulo,\n",
                "            salt=salt,\n",
                "        )\n",
                "        for width, multiplier, salt in zip(window_widths, multipliers, salts)\n",
                "    ]\n",
                "\n",
                "    for i, hasher in enumerate(hashers):\n",
                "        smallest_hash = skipped_u64_intermediary\n",
                "        smallest_count = 0\n",
                "        smallest_example = None\n",
                "        for rolling_intermediate_u64_hash in hasher:\n",
                "            new_smallest_hash = min(smallest_hash, rolling_intermediate_u64_hash)\n",
                "            if new_smallest_hash < smallest_hash:\n",
                "                smallest_count = 1\n",
                "                smallest_hash = new_smallest_hash\n",
                "                smallest_example = text[i : i + window_widths[i]]\n",
                "            elif new_smallest_hash == smallest_hash:\n",
                "                smallest_count += 1\n",
                "\n",
                "        smallest_hash &= skipped_final_hash  # Ensure we don't exceed the `uint32` range\n",
                "        fingerprint_hashes[i] = smallest_hash\n",
                "        fingerprint_weights[i] = smallest_count\n",
                "        fingerprint_ngrams[i] = smallest_example\n",
                "\n",
                "    return fingerprint_hashes, fingerprint_weights, fingerprint_ngrams\n",
                "\n",
                "\n",
                "rolling_count_min_sketch(\"abcde\", window_widths=[3, 4], multipliers=[257, 258], salts=[1, 2], modulo=4503599626977)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A good set of hyper-parameters for Min-Hashing binary text would be:\n",
                "\n",
                "- `window_widths`: ${3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 18, 21, 24, 27, 30}$ - 16 widths\n",
                "- `alphabet_size`: $256$ for ASCII & binary UTF-8 content\n",
                "- `ndim`: $16...1024$, something like 192 should be great for X/Twitter\n",
                "- `multipliers`: ${257, 258, 259, 260, 261, 262, ..., 1024 + 256}$\n",
                "\n",
                "When processing less usual inputs, like the DNA sequences, parameters may be different, e.g.:\n",
                "\n",
                "- `window_widths`: ${3, 6, 9, 12, 15, 30, 60, 120}$\n",
                "- `alphabet_size`: $4$ for DNA sequences\n",
                "- `ndim`: should be probably proportional to $√n$, where $n$ is the typical length of sequences\n",
                "- `multipliers`: ${5, 6, 7, 8, 9, ..., 4 * n + 1}$\n",
                "\n",
                "In every case, the `modulo` should be co-prime to the multiplier.\n",
                "The easiest option is to use a large prime, that can be obtained via:\n",
                "\n",
                "```python\n",
                "largest_prime_below(int(LARGEST_INTEGRAL_FLOAT) // max(multipliers) - (alphabet_size + 1))\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from typing import Tuple\n",
                "\n",
                "\n",
                "def jaccard_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
                "    if a.shape != b.shape:\n",
                "        raise ValueError(\"Fingerprints must have identical length\")\n",
                "\n",
                "    return float(np.mean(a == b))\n",
                "\n",
                "\n",
                "def weighted_jaccard_similarity(\n",
                "    a: Tuple[np.ndarray, np.ndarray],\n",
                "    b: Tuple[np.ndarray, np.ndarray],\n",
                ") -> float:\n",
                "    hashes_a, weights_a = a\n",
                "    hashes_b, weights_b = b\n",
                "\n",
                "    if hashes_a.shape != hashes_b.shape or weights_a.shape != weights_b.shape:\n",
                "        raise ValueError(\"Both fingerprints must have identical dimensions\")\n",
                "\n",
                "    magnitude_i = (weights_a * weights_b)[hashes_a == hashes_b].sum()\n",
                "    magnitude_a = (weights_a * weights_a).sum()\n",
                "    magnitude_b = (weights_b * weights_b).sum()\n",
                "    magnitude_u = magnitude_a + magnitude_b - magnitude_i\n",
                "\n",
                "    return float(magnitude_i) / float(magnitude_u)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's compute the rolling fingerprints:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 1,000,000 lines of mean length 128.64 characters\n"
                    ]
                }
            ],
            "source": [
                "textual_dataset_path = dataset_directory / \"leipzig1M.txt\"\n",
                "textual_dataset = open(textual_dataset_path, \"r\").read().casefold().strip()\n",
                "textual_lines = textual_dataset.split(\"\\n\")\n",
                "print(f\"Loaded {len(textual_lines):,} lines of mean length {sum(len(line) for line in textual_lines) / len(textual_lines):.2f} characters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Fingerprinting lines: 100%|██████████| 10000/10000 [00:43<00:00, 232.46line/s]\n"
                    ]
                }
            ],
            "source": [
                "from tqdm import tqdm\n",
                "from itertools import islice\n",
                "\n",
                "\n",
                "def take_first_n(iterable, n):\n",
                "    return islice(iterable, n)\n",
                "\n",
                "\n",
                "def keep_each_nth(iterable, k):\n",
                "    return (x for i, x in enumerate(iterable, 1) if i % k == 0)\n",
                "\n",
                "\n",
                "NDIM: int = 192\n",
                "consecutive_multipliers = list(range(256, 256 + NDIM))\n",
                "prime_multipliers = list(take_first_n(keep_each_nth(next_primes(257), 7), NDIM))\n",
                "\n",
                "window_widths = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 18, 21, 24, 27, 30]\n",
                "window_widths *= NDIM // len(window_widths)\n",
                "salts = range(1, NDIM + 1)  # Use different salts for each window width\n",
                "alphabet_size = 256\n",
                "largest_term = alphabet_size + max(salts)\n",
                "LARGEST_SAFE_MODULO = next(\n",
                "    prev_primes(int(LARGEST_INTEGRAL_FLOAT) // max(prime_multipliers) - largest_term)\n",
                ")\n",
                "HASH_DTYPE = np.uint64\n",
                "\n",
                "fingerprint_hashes = []\n",
                "fingerprint_counts = []\n",
                "fingerprint_ngrams = []\n",
                "\n",
                "DATASET_SIZE_LIMIT = 10_000\n",
                "\n",
                "default_min_sketcher = lambda line: count_min_sketch(\n",
                "    text=line,\n",
                "    window_widths=window_widths,\n",
                "    seeds=prime_multipliers,\n",
                "    hash_resolution=HASH_DTYPE,\n",
                ")\n",
                "# For Rabin-Karp rolling hashes we pass more parameters:\n",
                "default_rolling_sketcher = lambda line: rolling_count_min_sketch(\n",
                "    text=line,\n",
                "    window_widths=window_widths,\n",
                "    multipliers=prime_multipliers,\n",
                "    salts=salts,\n",
                "    modulo=LARGEST_SAFE_MODULO,\n",
                "    hash_resolution=HASH_DTYPE,\n",
                ")\n",
                "\n",
                "for line in tqdm(\n",
                "    textual_lines[:DATASET_SIZE_LIMIT], desc=\"Fingerprinting lines\", unit=\"line\"\n",
                "):\n",
                "    hashes, counts, ngrams = default_min_sketcher(line)\n",
                "    fingerprint_hashes.append(hashes)\n",
                "    fingerprint_counts.append(counts)\n",
                "    fingerprint_ngrams.append(ngrams)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's cross-reference the fingerprints counting the number of hash collisions without our test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dimension 0: 345 unique hashes, 0 collisions\n",
                        "Dimension 1: 1,366 unique hashes, 0 collisions\n",
                        "Dimension 2: 2,439 unique hashes, 0 collisions\n",
                        "Dimension 3: 3,400 unique hashes, 0 collisions\n",
                        "Dimension 4: 5,267 unique hashes, 0 collisions\n",
                        "Dimension 5: 6,297 unique hashes, 0 collisions\n",
                        "Dimension 6: 7,303 unique hashes, 0 collisions\n",
                        "Dimension 7: 8,027 unique hashes, 0 collisions\n",
                        "Dimension 8: 8,681 unique hashes, 0 collisions\n",
                        "Dimension 9: 9,059 unique hashes, 0 collisions\n",
                        "Dimension 10: 9,635 unique hashes, 0 collisions\n",
                        "Dimension 11: 9,887 unique hashes, 0 collisions\n",
                        "Dimension 12: 9,939 unique hashes, 0 collisions\n",
                        "Dimension 13: 9,941 unique hashes, 0 collisions\n",
                        "Dimension 14: 9,916 unique hashes, 0 collisions\n",
                        "Dimension 15: 9,879 unique hashes, 0 collisions\n",
                        "Dimension 16: 446 unique hashes, 0 collisions\n",
                        "Dimension 17: 1,335 unique hashes, 0 collisions\n",
                        "Dimension 18: 2,445 unique hashes, 0 collisions\n",
                        "Dimension 19: 3,641 unique hashes, 0 collisions\n",
                        "Dimension 20: 5,054 unique hashes, 0 collisions\n",
                        "Dimension 21: 6,272 unique hashes, 0 collisions\n",
                        "Dimension 22: 7,288 unique hashes, 0 collisions\n",
                        "Dimension 23: 8,114 unique hashes, 0 collisions\n",
                        "Dimension 24: 8,677 unique hashes, 0 collisions\n",
                        "Dimension 25: 8,934 unique hashes, 0 collisions\n",
                        "Dimension 26: 9,622 unique hashes, 0 collisions\n",
                        "Dimension 27: 9,891 unique hashes, 0 collisions\n",
                        "Dimension 28: 9,934 unique hashes, 0 collisions\n",
                        "Dimension 29: 9,947 unique hashes, 0 collisions\n",
                        "Dimension 30: 9,921 unique hashes, 0 collisions\n",
                        "Dimension 31: 9,875 unique hashes, 0 collisions\n",
                        "Dimension 32: 423 unique hashes, 0 collisions\n",
                        "Dimension 33: 1,331 unique hashes, 0 collisions\n",
                        "Dimension 34: 2,281 unique hashes, 0 collisions\n",
                        "Dimension 35: 3,579 unique hashes, 0 collisions\n",
                        "Dimension 36: 5,163 unique hashes, 0 collisions\n",
                        "Dimension 37: 6,388 unique hashes, 0 collisions\n",
                        "Dimension 38: 7,398 unique hashes, 0 collisions\n",
                        "Dimension 39: 7,918 unique hashes, 0 collisions\n",
                        "Dimension 40: 8,692 unique hashes, 0 collisions\n",
                        "Dimension 41: 8,988 unique hashes, 0 collisions\n",
                        "Dimension 42: 9,657 unique hashes, 0 collisions\n",
                        "Dimension 43: 9,855 unique hashes, 0 collisions\n",
                        "Dimension 44: 9,934 unique hashes, 0 collisions\n",
                        "Dimension 45: 9,944 unique hashes, 0 collisions\n",
                        "Dimension 46: 9,926 unique hashes, 0 collisions\n",
                        "Dimension 47: 9,872 unique hashes, 0 collisions\n",
                        "Dimension 48: 459 unique hashes, 0 collisions\n",
                        "Dimension 49: 875 unique hashes, 0 collisions\n",
                        "Dimension 50: 2,584 unique hashes, 0 collisions\n",
                        "Dimension 51: 3,709 unique hashes, 0 collisions\n",
                        "Dimension 52: 5,276 unique hashes, 0 collisions\n",
                        "Dimension 53: 6,460 unique hashes, 0 collisions\n",
                        "Dimension 54: 7,191 unique hashes, 0 collisions\n",
                        "Dimension 55: 7,973 unique hashes, 0 collisions\n",
                        "Dimension 56: 8,520 unique hashes, 0 collisions\n",
                        "Dimension 57: 9,010 unique hashes, 0 collisions\n",
                        "Dimension 58: 9,685 unique hashes, 0 collisions\n",
                        "Dimension 59: 9,859 unique hashes, 0 collisions\n",
                        "Dimension 60: 9,952 unique hashes, 0 collisions\n",
                        "Dimension 61: 9,939 unique hashes, 0 collisions\n",
                        "Dimension 62: 9,918 unique hashes, 0 collisions\n",
                        "Dimension 63: 9,871 unique hashes, 0 collisions\n",
                        "Dimension 64: 413 unique hashes, 0 collisions\n",
                        "Dimension 65: 1,101 unique hashes, 0 collisions\n",
                        "Dimension 66: 2,476 unique hashes, 0 collisions\n",
                        "Dimension 67: 3,692 unique hashes, 0 collisions\n",
                        "Dimension 68: 5,268 unique hashes, 0 collisions\n",
                        "Dimension 69: 6,433 unique hashes, 0 collisions\n",
                        "Dimension 70: 7,186 unique hashes, 0 collisions\n",
                        "Dimension 71: 8,179 unique hashes, 0 collisions\n",
                        "Dimension 72: 8,573 unique hashes, 0 collisions\n",
                        "Dimension 73: 8,992 unique hashes, 0 collisions\n",
                        "Dimension 74: 9,682 unique hashes, 0 collisions\n",
                        "Dimension 75: 9,841 unique hashes, 0 collisions\n",
                        "Dimension 76: 9,928 unique hashes, 0 collisions\n",
                        "Dimension 77: 9,918 unique hashes, 0 collisions\n",
                        "Dimension 78: 9,925 unique hashes, 0 collisions\n",
                        "Dimension 79: 9,876 unique hashes, 0 collisions\n",
                        "Dimension 80: 350 unique hashes, 0 collisions\n",
                        "Dimension 81: 1,266 unique hashes, 0 collisions\n",
                        "Dimension 82: 2,297 unique hashes, 0 collisions\n",
                        "Dimension 83: 3,625 unique hashes, 0 collisions\n",
                        "Dimension 84: 5,102 unique hashes, 0 collisions\n",
                        "Dimension 85: 6,497 unique hashes, 0 collisions\n",
                        "Dimension 86: 7,193 unique hashes, 0 collisions\n",
                        "Dimension 87: 8,038 unique hashes, 0 collisions\n",
                        "Dimension 88: 8,469 unique hashes, 0 collisions\n",
                        "Dimension 89: 9,035 unique hashes, 0 collisions\n",
                        "Dimension 90: 9,619 unique hashes, 0 collisions\n",
                        "Dimension 91: 9,865 unique hashes, 0 collisions\n",
                        "Dimension 92: 9,930 unique hashes, 0 collisions\n",
                        "Dimension 93: 9,931 unique hashes, 0 collisions\n",
                        "Dimension 94: 9,919 unique hashes, 0 collisions\n",
                        "Dimension 95: 9,865 unique hashes, 0 collisions\n",
                        "Dimension 96: 442 unique hashes, 0 collisions\n",
                        "Dimension 97: 1,179 unique hashes, 0 collisions\n",
                        "Dimension 98: 2,260 unique hashes, 0 collisions\n",
                        "Dimension 99: 3,904 unique hashes, 0 collisions\n",
                        "Dimension 100: 5,143 unique hashes, 0 collisions\n",
                        "Dimension 101: 6,179 unique hashes, 0 collisions\n",
                        "Dimension 102: 7,239 unique hashes, 0 collisions\n",
                        "Dimension 103: 7,790 unique hashes, 0 collisions\n",
                        "Dimension 104: 8,663 unique hashes, 0 collisions\n",
                        "Dimension 105: 9,092 unique hashes, 0 collisions\n",
                        "Dimension 106: 9,636 unique hashes, 0 collisions\n",
                        "Dimension 107: 9,840 unique hashes, 0 collisions\n",
                        "Dimension 108: 9,931 unique hashes, 0 collisions\n",
                        "Dimension 109: 9,937 unique hashes, 0 collisions\n",
                        "Dimension 110: 9,906 unique hashes, 0 collisions\n",
                        "Dimension 111: 9,875 unique hashes, 0 collisions\n",
                        "Dimension 112: 477 unique hashes, 0 collisions\n",
                        "Dimension 113: 1,019 unique hashes, 0 collisions\n",
                        "Dimension 114: 2,410 unique hashes, 0 collisions\n",
                        "Dimension 115: 3,583 unique hashes, 0 collisions\n",
                        "Dimension 116: 5,294 unique hashes, 0 collisions\n",
                        "Dimension 117: 6,508 unique hashes, 0 collisions\n",
                        "Dimension 118: 7,062 unique hashes, 0 collisions\n",
                        "Dimension 119: 8,001 unique hashes, 0 collisions\n",
                        "Dimension 120: 8,524 unique hashes, 0 collisions\n",
                        "Dimension 121: 8,972 unique hashes, 0 collisions\n",
                        "Dimension 122: 9,652 unique hashes, 0 collisions\n",
                        "Dimension 123: 9,854 unique hashes, 0 collisions\n",
                        "Dimension 124: 9,947 unique hashes, 0 collisions\n",
                        "Dimension 125: 9,943 unique hashes, 0 collisions\n",
                        "Dimension 126: 9,918 unique hashes, 0 collisions\n",
                        "Dimension 127: 9,871 unique hashes, 0 collisions\n",
                        "Dimension 128: 436 unique hashes, 0 collisions\n",
                        "Dimension 129: 1,221 unique hashes, 0 collisions\n",
                        "Dimension 130: 2,291 unique hashes, 0 collisions\n",
                        "Dimension 131: 3,850 unique hashes, 0 collisions\n",
                        "Dimension 132: 5,053 unique hashes, 0 collisions\n",
                        "Dimension 133: 6,374 unique hashes, 0 collisions\n",
                        "Dimension 134: 7,393 unique hashes, 0 collisions\n",
                        "Dimension 135: 8,098 unique hashes, 0 collisions\n",
                        "Dimension 136: 8,602 unique hashes, 0 collisions\n",
                        "Dimension 137: 9,073 unique hashes, 0 collisions\n",
                        "Dimension 138: 9,695 unique hashes, 0 collisions\n",
                        "Dimension 139: 9,873 unique hashes, 0 collisions\n",
                        "Dimension 140: 9,924 unique hashes, 0 collisions\n",
                        "Dimension 141: 9,940 unique hashes, 0 collisions\n",
                        "Dimension 142: 9,920 unique hashes, 0 collisions\n",
                        "Dimension 143: 9,870 unique hashes, 0 collisions\n",
                        "Dimension 144: 473 unique hashes, 0 collisions\n",
                        "Dimension 145: 1,208 unique hashes, 0 collisions\n",
                        "Dimension 146: 2,305 unique hashes, 0 collisions\n",
                        "Dimension 147: 3,852 unique hashes, 0 collisions\n",
                        "Dimension 148: 4,905 unique hashes, 0 collisions\n",
                        "Dimension 149: 6,541 unique hashes, 0 collisions\n",
                        "Dimension 150: 7,204 unique hashes, 0 collisions\n",
                        "Dimension 151: 7,946 unique hashes, 0 collisions\n",
                        "Dimension 152: 8,543 unique hashes, 0 collisions\n",
                        "Dimension 153: 9,029 unique hashes, 0 collisions\n",
                        "Dimension 154: 9,591 unique hashes, 0 collisions\n",
                        "Dimension 155: 9,827 unique hashes, 0 collisions\n",
                        "Dimension 156: 9,933 unique hashes, 0 collisions\n",
                        "Dimension 157: 9,938 unique hashes, 0 collisions\n",
                        "Dimension 158: 9,912 unique hashes, 0 collisions\n",
                        "Dimension 159: 9,871 unique hashes, 0 collisions\n",
                        "Dimension 160: 406 unique hashes, 0 collisions\n",
                        "Dimension 161: 1,297 unique hashes, 0 collisions\n",
                        "Dimension 162: 2,575 unique hashes, 0 collisions\n",
                        "Dimension 163: 3,728 unique hashes, 0 collisions\n",
                        "Dimension 164: 5,267 unique hashes, 0 collisions\n",
                        "Dimension 165: 6,177 unique hashes, 0 collisions\n",
                        "Dimension 166: 7,355 unique hashes, 0 collisions\n",
                        "Dimension 167: 8,058 unique hashes, 0 collisions\n",
                        "Dimension 168: 8,525 unique hashes, 0 collisions\n",
                        "Dimension 169: 9,061 unique hashes, 0 collisions\n",
                        "Dimension 170: 9,665 unique hashes, 0 collisions\n",
                        "Dimension 171: 9,870 unique hashes, 0 collisions\n",
                        "Dimension 172: 9,921 unique hashes, 0 collisions\n",
                        "Dimension 173: 9,940 unique hashes, 0 collisions\n",
                        "Dimension 174: 9,918 unique hashes, 0 collisions\n",
                        "Dimension 175: 9,881 unique hashes, 0 collisions\n",
                        "Dimension 176: 373 unique hashes, 0 collisions\n",
                        "Dimension 177: 1,308 unique hashes, 0 collisions\n",
                        "Dimension 178: 2,357 unique hashes, 0 collisions\n",
                        "Dimension 179: 3,728 unique hashes, 0 collisions\n",
                        "Dimension 180: 5,421 unique hashes, 0 collisions\n",
                        "Dimension 181: 6,246 unique hashes, 0 collisions\n",
                        "Dimension 182: 7,419 unique hashes, 0 collisions\n",
                        "Dimension 183: 8,200 unique hashes, 0 collisions\n",
                        "Dimension 184: 8,448 unique hashes, 0 collisions\n",
                        "Dimension 185: 8,999 unique hashes, 0 collisions\n",
                        "Dimension 186: 9,673 unique hashes, 0 collisions\n",
                        "Dimension 187: 9,893 unique hashes, 0 collisions\n",
                        "Dimension 188: 9,941 unique hashes, 0 collisions\n",
                        "Dimension 189: 9,949 unique hashes, 0 collisions\n",
                        "Dimension 190: 9,921 unique hashes, 0 collisions\n",
                        "Dimension 191: 9,861 unique hashes, 0 collisions\n"
                    ]
                }
            ],
            "source": [
                "from typing import Dict, Set\n",
                "\n",
                "for dim in range(len(window_widths)):\n",
                "    hash_to_ngram: Dict[int, str] = {}\n",
                "    hash_collisions: Set[int] = set()\n",
                "    for hashes, ngrams in zip(fingerprint_hashes, fingerprint_ngrams):\n",
                "        hash_value = hashes[dim]\n",
                "        ngram_value = ngrams[dim]\n",
                "        if hash_value not in hash_to_ngram:\n",
                "            hash_to_ngram[hash_value] = ngram_value\n",
                "        elif hash_to_ngram[hash_value] != ngram_value:\n",
                "            hash_collisions.add(hash_value)\n",
                "\n",
                "    print(f\"Dimension {dim}: {len(hash_to_ngram):,} unique hashes, {len(hash_collisions):,} collisions\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's estimate Recall @ 1, but before we do that - let's find a way to highlight N-gram matches between strings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(\"A short string<span style='color:#ffff00'> <span style='color:#0080ff'>wit</span></span>h an <span style='color:#ff0000'><span style='color:#ff8000'>n<span style='color:#00ff00'><span style='color:#ff00ff'>-gr</span>a</span>m</span></span>\",\n",
                            " \"Longer strings<span style='color:#ffff00'> <span style='color:#0080ff'>wit</span></span>h different <span style='color:#ff0000'><span style='color:#ff8000'>n<span style='color:#00ff00'><span style='color:#ff00ff'>-gr</span>a</span>m</span></span>s\")"
                        ]
                    },
                    "execution_count": 57,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from typing import Tuple\n",
                "from IPython.display import HTML\n",
                "import numpy as np\n",
                "\n",
                "HTML_COLORS = [\n",
                "    \"#ff0000\",\n",
                "    \"#ff8000\",\n",
                "    \"#ffff00\",\n",
                "    \"#00ff00\",\n",
                "    \"#0080ff\",\n",
                "    \"#ff00ff\",\n",
                "    \"#800080\",\n",
                "]\n",
                "ASCII_COLORS = [\n",
                "    \"\\033[38;5;196m\",  # red\n",
                "    \"\\033[38;5;208m\",  # orange\n",
                "    \"\\033[38;5;226m\",  # yellow\n",
                "    \"\\033[38;5;082m\",  # green\n",
                "    \"\\033[38;5;039m\",  # blue\n",
                "    \"\\033[38;5;201m\",  # magenta\n",
                "    \"\\033[38;5;129m\",  # purple\n",
                "]\n",
                "\n",
                "\n",
                "def color_code_matches(\n",
                "    query_text: str,\n",
                "    document_text: str,\n",
                "    query_hashes: np.ndarray,\n",
                "    document_hashes: np.ndarray,\n",
                "    query_ngrams: np.ndarray,\n",
                "    document_ngrams: np.ndarray,\n",
                "    *,\n",
                "    html: bool = True,\n",
                ") -> Tuple[str, str]:\n",
                "    \"\"\"Highlight matching n‑grams / hash‑collisions in the two texts.\"\"\"\n",
                "\n",
                "    COLOR_ARRAY = (\n",
                "        [f\"<span style='color:{hex_}'>\" for hex_ in HTML_COLORS]\n",
                "        if html\n",
                "        else ASCII_COLORS\n",
                "    )\n",
                "    COLOR_COLLISION = (\n",
                "        \"<span style='color:#888888'>\" if html else \"\\033[38;5;244m\"\n",
                "    )  # grey\n",
                "    COLOR_RESET = \"</span>\" if html else \"\\033[0m\"\n",
                "\n",
                "    def number_of_matches_in_dimension(dim: int) -> int:\n",
                "        if len(query_ngrams[dim]) == 0 or len(document_ngrams[dim]) == 0:\n",
                "            return 0\n",
                "        return min(\n",
                "            query_text.count(query_ngrams[dim]),\n",
                "            document_text.count(document_ngrams[dim]),\n",
                "        )\n",
                "\n",
                "    def ngram_length_in_dimension(dim: int) -> int:\n",
                "        return len(query_ngrams[dim]) if dim < len(query_ngrams) else 0\n",
                "\n",
                "    all_dims = [\n",
                "        d for d in range(len(query_hashes)) if number_of_matches_in_dimension(d)\n",
                "    ]\n",
                "    all_dims.sort(key=ngram_length_in_dimension, reverse=True)\n",
                "\n",
                "    color_index = 0\n",
                "    for dim in all_dims:\n",
                "        if number_of_matches_in_dimension(dim) == 0:\n",
                "            continue\n",
                "\n",
                "        is_hash_eq = query_hashes[dim] == document_hashes[dim]\n",
                "        is_ngram_eq = query_ngrams[dim] == document_ngrams[dim]\n",
                "        token = query_ngrams[dim]\n",
                "        assert token, \"N‑gram must not be empty\"\n",
                "\n",
                "        if is_ngram_eq:\n",
                "            color_tag = COLOR_ARRAY[color_index % len(COLOR_ARRAY)]\n",
                "            replacement = f\"{color_tag}{token}{COLOR_RESET}\"\n",
                "            color_index += 1\n",
                "        elif is_hash_eq:\n",
                "            replacement = f\"{COLOR_COLLISION}{token}{COLOR_RESET}\"\n",
                "        else:\n",
                "            continue\n",
                "\n",
                "        query_text = query_text.replace(token, replacement)\n",
                "        document_text = document_text.replace(token, replacement)\n",
                "\n",
                "    return query_text, document_text\n",
                "\n",
                "\n",
                "query_text = \"A short string with an n-gram\"\n",
                "document_text = \"Longer strings with different n-grams\"\n",
                "query_hashes, query_weights, query_ngrams = default_min_sketcher(query_text)\n",
                "document_hashes, document_weights, document_ngrams = default_min_sketcher(document_text)\n",
                "color_code_matches(\n",
                "    query_text=query_text,\n",
                "    document_text=document_text,\n",
                "    query_hashes=query_hashes,\n",
                "    document_hashes=document_hashes,\n",
                "    query_ngrams=query_ngrams,\n",
                "    document_ngrams=document_ngrams,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Searching: 100%|██████████| 10/10 [00:00<00:00, 26.05doc/s]\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style='font-family:monospace'>Matched query 0 with document 1,559 with score 0.0677<br/>a rebel statement sent to lisbon from jamba said 86 government soldie<span style='color:#ff00ff'>rs </span>and 13 gu<span style='color:#ff0000'>erril<span style='color:#ff8000'>las<span style='color:#ffff00'><span style='color:#00ff00'> were</span> </span>ki</span>l<span style='color:#0080ff'>led </span></span>in <span style='color:#ff0000'>the</span> f<span style='color:#800080'>igh</span>ting that ended jan. 3. it said <span style='color:#ff0000'>the</span> rebel forces sill held mavinga.<br/>hou<span style='color:#ff00ff'>rs </span>later, six leftist gu<span style='color:#ff0000'>erril<span style='color:#ff8000'>las<span style='color:#ffff00'><span style='color:#00ff00'> were</span> </span>ki</span>l<span style='color:#0080ff'>led </span></span>in a battle with a special army brigade created to f<span style='color:#800080'>igh</span>t <span style='color:#ff0000'>the</span> rebels.<br/>Matched query 1 with document 6,376 with score 0.0573<br/>auth<span style='color:#ff00ff'><span style='color:#800080'>ori</span>t</span>ies last week issued a vacate order for a club in<span style='color:#ff0000'> ma<span style='color:#00ff00'>nh<span style='color:#ff0000'>att</span></span></span>an<span style='color:#0080ff'> and</span> closed another in th<span style='color:#ff8000'>e <span style='color:#ffff00'>bronx</span></span>.<br/>auth<span style='color:#ff00ff'><span style='color:#800080'>ori</span>t</span>ies raided 15 suspected boiler room sites monday in<span style='color:#ff0000'> ma<span style='color:#00ff00'>nh<span style='color:#ff0000'>att</span></span></span>an, th<span style='color:#ff8000'>e <span style='color:#ffff00'>bronx</span></span>, brooklyn<span style='color:#0080ff'> and</span> queens,<span style='color:#0080ff'> and</span> on long island in hempstead<span style='color:#0080ff'> and</span> massapequa.<br/>Matched query 2 with document 2,540 with score 0.0625<br/>at the first pan am bankruptcy hearing, for example, a<span style='color:#ff0000'><span style='color:#ff8000'><span style='color:#0080ff'>t le</span>ast</span> fi</span>ve airlines<span style='color:#ffff00'><span style='color:#00ff00'> were</span> </span>represent<span style='color:#ff00ff'>ed.</span><br/>a<span style='color:#ff0000'><span style='color:#ff8000'><span style='color:#0080ff'>t le</span>ast</span> fi</span>ve businesses<span style='color:#ffff00'><span style='color:#00ff00'> were</span> </span>destroyed and 15<span style='color:#ffff00'><span style='color:#00ff00'> were</span> </span>damag<span style='color:#ff00ff'>ed.</span><br/>Matched query 3 with document 3,331 with score 0.0417<br/>mr. neigum, poker-faced duri<span style='color:#ff0000'><span style='color:#00ff00'>ng </span>the </span><span style='color:#ff8000'>di<span style='color:#ffff00'>ffic</span></span>ult task,<span style='color:#0080ff'> ma</span>nages a 46-second showing.<br/>although they appreciate it, it's<span style='color:#0080ff'> ma</span>ki<span style='color:#ff0000'><span style='color:#00ff00'>ng </span>the </span>day-to-day operations very <span style='color:#ff8000'>di<span style='color:#ffff00'>ffic</span></span>ult.<br/>Matched query 4 with document 7,018 with score 0.0469<br/>this, co<span style='color:#ff0000'><span style='color:#ff8000'><span style='color:#ffff00'>mbined w</span>i</span>th </span>the container division talks, suggests the group's bankers might be considering an orderly disposal of all assets.<br/>that, co<span style='color:#ff0000'><span style='color:#ff8000'><span style='color:#ffff00'>mbined w</span>i</span>th </span>lower prices, could get gnp back up to zero growth during the first quarter of 1991, he says.<br/>Matched query 5 with document 6,679 with score 0.0573<br/>she told the post i<span style='color:#ff0000'>n a<span style='color:#ff8000'>n <span style='color:#ffff00'><span style='color:#00ff00'>i<span style='color:#ff00ff'>nter</span></span>v</span></span></span>iew published sunday that some of the money may have become \"mingled\" into improvements on her home that included a swimming pool, a $2,500 wide-screen televis<span style='color:#0080ff'>ion </span>and renovations to her basement.<br/>chancellor helmut kohl said i<span style='color:#ff0000'>n a<span style='color:#ff8000'>n <span style='color:#ffff00'><span style='color:#00ff00'>i<span style='color:#ff00ff'>nter</span></span>v</span></span></span>iew with the ard televis<span style='color:#0080ff'>ion </span>network that he was \"alarmed\" by the republicans' showing in west berlin.<br/>Matched query 6 with document 3,322 with score 0.0469<br/>acco<span style='color:#ff0000'>r<span style='color:#ff8000'>ding t</span></span>o a stud<span style='color:#0080ff'>y b</span>y the m<span style='color:#00ff00'>ars</span>hall institute, the average nasa employee's age<span style='color:#ffff00'> in </span>1963 was 30; now most of its senior and middle-managers will be eligible to retire<span style='color:#ffff00'> in </span>five ye<span style='color:#00ff00'>ars</span>.<br/>fifteen ye<span style='color:#00ff00'>ars</span> after the first oil embargo, the u.s. economy has stopped making gains<span style='color:#ffff00'> in </span>energy efficiency, acco<span style='color:#ff0000'>r<span style='color:#ff8000'>ding t</span></span>o experts, who add that the country ma<span style='color:#0080ff'>y b</span>e losing ground instead.<br/>Matched query 7 with document 8,828 with score 0.0990<br/>preston tisch, 62, is <span style='color:#ffff00'><span style='color:#00ff00'>president</span> a</span><span style='color:#ff0000'>nd </span>co-chie<span style='color:#ff0000'>f execu<span style='color:#ff8000'>tive o<span style='color:#0080ff'>f<span style='color:#ff00ff'>fice</span></span>r</span> o</span>f loews<span style='color:#800080'> cor</span>p. a<span style='color:#ff0000'>nd </span>is a former postmaster general.<br/>arnold staloff, past <span style='color:#00ff00'>president</span> of the new york commodity exchange, on tuesday was named <span style='color:#ffff00'><span style='color:#00ff00'>president</span> a</span><span style='color:#ff0000'>nd </span>chie<span style='color:#ff0000'>f execu<span style='color:#ff8000'>tive o<span style='color:#0080ff'>f<span style='color:#ff00ff'>fice</span></span>r</span> o</span>f bloom staloff<span style='color:#800080'> cor</span>p., a philadelphia securities trading firm.<br/>Matched query 8 with document 7,057 with score 0.0521<br/>\"we'<span style='color:#ff0000'><span style='color:#ff8000'>re d<span style='color:#ffff00'>ealing </span></span></span>w<span style='color:#0080ff'>ith</span> an owner who couldn't give a rip.<span style='color:#00ff00'> they</span> cut off her mail and sh<span style='color:#ff00ff'>e g</span>ot a post office box.\" starting friday, an animal-control officer is accompanying finster on his route.<br/>\"once the borrowers knows<span style='color:#00ff00'> they</span> a<span style='color:#ff0000'><span style='color:#ff8000'>re d<span style='color:#ffff00'>ealing </span></span></span>w<span style='color:#0080ff'>ith</span> th<span style='color:#ff00ff'>e g</span>overnment, the likelihood of delinquency increases tremendously,\" he said.<br/>Matched query 9 with document 1,011 with score 0.0573<br/>asked if<span style='color:#ff00ff'> he </span>might bring <span style='color:#ff0000'>the</span> world leaders to texas, possibly to san antonio, <span style='color:#ff0000'>t<span style='color:#ff8000'>h<span style='color:#ffff00'>e<span style='color:#00ff00'> pres</span></span>ide</span></span>nt remarked, \"<span style='color:#0080ff'>t<span style='color:#800080'>hat'</span></span>s a distinct possibility.<br/>\"<span style='color:#0080ff'>t<span style='color:#800080'>hat'</span></span>s not <span style='color:#ff0000'>the</span> issue,\"<span style='color:#ff00ff'> he </span>said. \"i got enough votes to become <span style='color:#ff0000'>t<span style='color:#ff8000'>h<span style='color:#ffff00'>e<span style='color:#00ff00'> pres</span></span>ide</span></span>nt.</pre>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from tqdm import tqdm\n",
                "from IPython.display import display\n",
                "\n",
                "QUERIES_TO_COMPARE = 10\n",
                "\n",
                "log_lines = []\n",
                "\n",
                "for i, query_hashes, query_counts, query_ngrams in tqdm(zip(\n",
                "    range(QUERIES_TO_COMPARE),\n",
                "    fingerprint_hashes[:QUERIES_TO_COMPARE],\n",
                "    fingerprint_counts[:QUERIES_TO_COMPARE],\n",
                "    fingerprint_ngrams[:QUERIES_TO_COMPARE],\n",
                "), desc=\"Searching\", unit=\"doc\", total=QUERIES_TO_COMPARE):\n",
                "    \n",
                "    # Compare with all other fingerprints\n",
                "    best_score, best_index = 0.0, -1\n",
                "    for j, dataset_hashes, dataset_counts, dataset_ngrams in zip(\n",
                "        range(len(fingerprint_hashes)),\n",
                "        fingerprint_hashes,\n",
                "        fingerprint_counts,\n",
                "        fingerprint_ngrams,\n",
                "    ):\n",
                "        if i == j:\n",
                "            continue\n",
                "\n",
                "        score = jaccard_similarity(query_hashes, dataset_hashes)\n",
                "        if score > best_score:\n",
                "            best_score = score\n",
                "            best_index = j\n",
                "\n",
                "    query = textual_lines[i]\n",
                "    doc = textual_lines[best_index]\n",
                "    colored_query, colored_doc = color_code_matches(\n",
                "        query_text=query,\n",
                "        document_text=doc,\n",
                "        query_hashes=query_hashes,\n",
                "        document_hashes=fingerprint_hashes[best_index],\n",
                "        query_ngrams=query_ngrams,\n",
                "        document_ngrams=fingerprint_ngrams[best_index],\n",
                "    )\n",
                "    log_lines.extend([\n",
                "        f\"Matched query {i:,} with document {best_index:,} with score {best_score:.4f}\",\n",
                "        colored_query,\n",
                "        colored_doc,\n",
                "    ])\n",
                "    \n",
                "concatenated_log = \"<br/>\".join(log_lines)\n",
                "monospaced_log = HTML(f\"<pre style='font-family:monospace'>{concatenated_log}</pre>\")\n",
                "display(monospaced_log)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Searching: 100%|██████████| 10/10 [00:00<00:00, 16.21doc/s]\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style='font-family:monospace'>Matched query 0 with document 4,683 with score 0.0350<br/>a rebel statement sent to lisbon from jamba said 86 g<span style='color:#ff0000'>o<span style='color:#ff8000'><span style='color:#ffff00'>vernme</span>n</span>t </span>soldiers<span style='color:#00ff00'> and</span> 13 guerrillas we<span style='color:#0080ff'>re </span>killed in the fighting that ended jan. 3. it said the rebel forces sill held mavinga.<br/>sapoa, nicaragua _ rebel<span style='color:#00ff00'> and</span> g<span style='color:#ff0000'>o<span style='color:#ff8000'><span style='color:#ffff00'>vernme</span>n</span>t </span>negotiators began hammering out the details of a 60-day cease-fi<span style='color:#0080ff'>re </span>outlined in the peace accord signed last week.<br/>Matched query 1 with document 4,907 with score 0.0424<br/>a<span style='color:#ff0000'><span style='color:#ff8000'>uthori<span style='color:#ffff00'>tie<span style='color:#0080ff'>s la</span>st</span> we</span><span style='color:#ff00ff'>ek </span></span>issued a vacate order for a club in manhattan<span style='color:#00ff00'> and</span> closed another in the bronx.<br/>a<span style='color:#ff0000'><span style='color:#ff8000'>uthori<span style='color:#ffff00'>tie<span style='color:#0080ff'>s la</span>st</span> we</span><span style='color:#ff00ff'>ek </span></span>announced the seizure of nearly 100 tons of steel tubing<span style='color:#00ff00'> and</span> other parts about to be shipped out of italy to jordan to be transported overland from there, presumably to an assembly point near baghdad.<br/>Matched query 2 with document 4,939 with score 0.0396<br/>a<span style='color:#ff0000'>t the<span style='color:#ff8000'> fir</span></span>st pan am bankruptcy hearing, for example, at least five airlines were represented.<br/>a<span style='color:#ff0000'>t the<span style='color:#ff8000'> fir</span></span>st show, como won for best vocal performance by a male.<br/>Matched query 3 with document 9,924 with score 0.0233<br/>mr. neigum, poker-faced duri<span style='color:#ffff00'>ng </span>the<span style='color:#00ff00'> di</span><span style='color:#ff0000'><span style='color:#ff8000'>ffic</span>ult </span>task, manages a 46-second showing.<br/>there is a series of really important and<span style='color:#00ff00'> di</span><span style='color:#ff0000'><span style='color:#ff8000'>ffic</span>ult </span>issues of european policy cryi<span style='color:#ffff00'>ng </span>out for it to address.<br/>Matched query 4 with document 7,018 with score 0.0688<br/>this, co<span style='color:#ff0000'><span style='color:#ff8000'><span style='color:#ffff00'>mbined w</span>i</span>th </span>the container division talks, suggests the group's bankers might be considering an orderly disposal of all assets.<br/>that, co<span style='color:#ff0000'><span style='color:#ff8000'><span style='color:#ffff00'>mbined w</span>i</span>th </span>lower prices, could get gnp back up to zero growth during the first quarter of 1991, he says.<br/>Matched query 5 with document 883 with score 0.0523<br/>she told the post i<span style='color:#ff0000'>n a<span style='color:#ff8000'>n <span style='color:#ffff00'>i<span style='color:#0080ff'>nter</span>v</span></span></span>iew published sunday that some of the money may have become \"mingled\" into improvement<span style='color:#ff00ff'>s on</span> her home that included a swimming pool, a $2,500 wide-screen televis<span style='color:#00ff00'>ion </span>and renovations to her basement.<br/>i<span style='color:#ff0000'>n a<span style='color:#ff8000'>n <span style='color:#ffff00'>i<span style='color:#0080ff'>nter</span>v</span></span></span>iew with the washington post in early october, the secretary said the fed may be slightly more i<span style='color:#0080ff'>nter</span>ested in curbing inflat<span style='color:#00ff00'>ion </span>than the administrat<span style='color:#00ff00'>ion </span>is, while the administrat<span style='color:#00ff00'>ion </span>may put slightly more emphasi<span style='color:#ff00ff'>s on</span> spurring economic growth.<br/>Matched query 6 with document 7,774 with score 0.0331<br/>acco<span style='color:#ff0000'>r<span style='color:#ff8000'>ding t</span></span>o a study by the marshall institute<span style='color:#0080ff'>, t</span>he average nasa employee's age in 1963 was 30; now most of its senior<span style='color:#ffff00'> and</span> middle-managers will be eligible to reti<span style='color:#00ff00'>re </span>in five years.<br/>acco<span style='color:#ff0000'>r<span style='color:#ff8000'>ding t</span></span>o the royal palace<span style='color:#0080ff'>, t</span>he crackdown left 10 people dead<span style='color:#ffff00'> and</span> 107 injured throughout the country on april 6. but witnesses said at least 200 people we<span style='color:#00ff00'>re </span>killed in katmandu alone.<br/>Matched query 7 with document 4,990 with score 0.0621<br/>preston tisch, 62, is <span style='color:#ffff00'><span style='color:#0080ff'>president</span> a</span>nd co-<span style='color:#00ff00'>c<span style='color:#800080'>hi<span style='color:#ff8000'>ef </span></span>exec</span>u<span style='color:#ff0000'><span style='color:#ff8000'>ti<span style='color:#ff00ff'>ve of<span style='color:#ff0000'>fice</span></span>r</span> of</span> loews corp. and is a former postmaster general.<br/>thomas m. egan, <span style='color:#ffff00'><span style='color:#0080ff'>president</span> a</span>nd <span style='color:#00ff00'>c<span style='color:#800080'>hi<span style='color:#ff8000'>ef </span></span>exec</span>u<span style='color:#ff0000'><span style='color:#ff8000'>ti<span style='color:#ff00ff'>ve of<span style='color:#ff0000'>fice</span></span>r</span> of</span> stotler group, said the company had been in negotiations with the creditors in efforts to reach a solution.<br/>Matched query 8 with document 7,057 with score 0.0496<br/>\"we'<span style='color:#ff0000'><span style='color:#ff8000'>re d<span style='color:#ffff00'>ealing </span></span></span>w<span style='color:#0080ff'>ith</span> an owner who couldn't give a rip.<span style='color:#00ff00'> they</span> cut off her mail and sh<span style='color:#ff00ff'>e g</span>ot a post office box.\" starting friday, an animal-control officer is accompanying finster on his route.<br/>\"once the borrowers knows<span style='color:#00ff00'> they</span> a<span style='color:#ff0000'><span style='color:#ff8000'>re d<span style='color:#ffff00'>ealing </span></span></span>w<span style='color:#0080ff'>ith</span> th<span style='color:#ff00ff'>e g</span>overnment, the likelihood of delinquency increases tremendously,\" he said.<br/>Matched query 9 with document 8,003 with score 0.0340<br/>asked if he m<span style='color:#0080ff'>igh</span>t bri<span style='color:#ff0000'><span style='color:#ff8000'>n<span style='color:#ffff00'><span style='color:#00ff00'>g the </span>w</span></span>o</span>rld lea<span style='color:#ff00ff'>der</span>s to texas, possibly to san antonio, the president remarked, \"that's a distinct possibility.<br/>about 8,800 firef<span style='color:#0080ff'>igh</span>ters are battli<span style='color:#ff0000'><span style='color:#ff8000'>n<span style='color:#ffff00'><span style='color:#00ff00'>g the </span>w</span></span>o</span>rst of some 200 blazes in the west and alaska, 16 of them consi<span style='color:#ff00ff'>der</span>ed major fires. nearly a third of the manpower is in yellowstone national park, where fires have charred 140,000 acres.</pre>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from tqdm import tqdm\n",
                "from IPython.display import display\n",
                "\n",
                "QUERIES_TO_COMPARE = 10\n",
                "\n",
                "log_lines = []\n",
                "\n",
                "for i, query_hashes, query_counts, query_ngrams in tqdm(\n",
                "    zip(\n",
                "        range(QUERIES_TO_COMPARE),\n",
                "        fingerprint_hashes[:QUERIES_TO_COMPARE],\n",
                "        fingerprint_counts[:QUERIES_TO_COMPARE],\n",
                "        fingerprint_ngrams[:QUERIES_TO_COMPARE],\n",
                "    ),\n",
                "    desc=\"Searching\",\n",
                "    unit=\"doc\",\n",
                "    total=QUERIES_TO_COMPARE,\n",
                "):\n",
                "\n",
                "    # Compare with all other fingerprints\n",
                "    best_score, best_index = 0.0, -1\n",
                "    for j, dataset_hashes, dataset_counts, dataset_ngrams in zip(\n",
                "        range(len(fingerprint_hashes)),\n",
                "        fingerprint_hashes,\n",
                "        fingerprint_counts,\n",
                "        fingerprint_ngrams,\n",
                "    ):\n",
                "        if i == j:\n",
                "            continue\n",
                "\n",
                "        score = weighted_jaccard_similarity(\n",
                "            (query_hashes, query_counts),\n",
                "            (dataset_hashes, dataset_counts),\n",
                "        )\n",
                "        if score > best_score:\n",
                "            best_score = score\n",
                "            best_index = j\n",
                "\n",
                "    query = textual_lines[i]\n",
                "    doc = textual_lines[best_index]\n",
                "    colored_query, colored_doc = color_code_matches(\n",
                "        query_text=query,\n",
                "        document_text=doc,\n",
                "        query_hashes=query_hashes,\n",
                "        document_hashes=fingerprint_hashes[best_index],\n",
                "        query_ngrams=query_ngrams,\n",
                "        document_ngrams=fingerprint_ngrams[best_index],\n",
                "    )\n",
                "    log_lines.extend(\n",
                "        [\n",
                "            f\"Matched query {i:,} with document {best_index:,} with score {best_score:.4f}\",\n",
                "            colored_query,\n",
                "            colored_doc,\n",
                "        ]\n",
                "    )\n",
                "\n",
                "concatenated_log = \"<br/>\".join(log_lines)\n",
                "monospaced_log = HTML(f\"<pre style='font-family:monospace'>{concatenated_log}</pre>\")\n",
                "display(monospaced_log)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Min-Hash Fingerprinting DNA & Protein Sequences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": [
                "dna_dataset_path = dataset_directory / \"acgt_10k.txt\"\n",
                "dna_dataset = open(dna_dataset_path, \"r\").read().strip()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "StringZilla",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
